{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Rankst Tag Recommendation System with SBERT Semantic Evaluation\n",
        "\n",
        "This notebook adds semantic evaluation to the existing rankst_k tag recommendation results.\n",
        "It loads pre-computed rankst results (k=1,2,3) and applies SBERT-based semantic similarity evaluation\n",
        "to compute semantic hits instead of just exact string matches.\n",
        "\n",
        "**Key features:**\n",
        "- Loads existing rankst_k results (without SBERT)\n",
        "- Uses SBERT (Sentence-BERT) for semantic similarity evaluation\n",
        "- Compares exact hits vs semantic hits\n",
        "- Computes precision, recall, F1 metrics with semantic evaluation\n",
        "- Analyzes improvement over exact string matching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from collections import Counter\n",
        "import math\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load Existing Rankst Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load existing rankst results for k=1,2,3\n",
        "k_values = [1, 2, 3]\n",
        "rankst_results = {}\n",
        "\n",
        "for k in k_values:\n",
        "    results_file = f'eval/rankst_k{k}_eval.json'\n",
        "    if os.path.exists(results_file):\n",
        "        print(f\"Loading rankst results for k={k}...\")\n",
        "        with open(results_file, 'r') as f:\n",
        "            rankst_results[k] = json.load(f)\n",
        "        print(f\"Loaded {len(rankst_results[k]['evaluation_details'])} test cases for k={k}\")\n",
        "    else:\n",
        "        print(f\"Warning: {results_file} not found\")\n",
        "\n",
        "print(f\"\\nLoaded results for k values: {list(rankst_results.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display current metrics summary\n",
        "print(\"=== Current Rankst Results Summary (Exact Matching) ===\")\n",
        "print(\"k | Precision@10 | Recall@10 | F1@10\")\n",
        "print(\"--|-------------|-----------|-------\")\n",
        "for k in sorted(rankst_results.keys()):\n",
        "    metrics = rankst_results[k]['metrics']\n",
        "    p = metrics['precision_10']['mean']\n",
        "    r = metrics['recall_10']['mean']\n",
        "    f = metrics['f1_10']['mean']\n",
        "    print(f\"{k} | {p:.4f}      | {r:.4f}    | {f:.4f}\")\n",
        "\n",
        "# Show example result structure\n",
        "example_result = rankst_results[1]['evaluation_details'][0]\n",
        "print(\"\\n=== Example Result Structure ===\")\n",
        "print(f\"Sound ID: {example_result['sound_id']}\")\n",
        "print(f\"Title: {example_result['title']}\")\n",
        "print(f\"Input tags (k={example_result['k']}): {example_result['input_tags_k']}\")\n",
        "print(f\"Recommended tags: {example_result['recommended_tags'][:5]}...\")\n",
        "print(f\"Ground truth tags: {example_result['ground_truth_tags']}\")\n",
        "print(f\"Exact hits: {example_result['hits']}\")\n",
        "print(f\"Exact Precision@10: {example_result['precision_10']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Initialize SBERT Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SBERT model for semantic similarity\n",
        "print(\"Loading SBERT model...\")\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"SBERT model loaded successfully!\")\n",
        "\n",
        "# Semantic similarity threshold (adjustable)\n",
        "SIMILARITY_THRESHOLD = 0.7\n",
        "print(f\"Semantic similarity threshold: {SIMILARITY_THRESHOLD}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all unique tags from all k values for SBERT encoding\n",
        "print(\"Collecting all unique tags for SBERT encoding...\")\n",
        "\n",
        "all_predicted_tags = set()\n",
        "all_ground_truth_tags = set()\n",
        "\n",
        "for k in rankst_results.keys():\n",
        "    for result in rankst_results[k]['evaluation_details']:\n",
        "        # Add predicted tags\n",
        "        all_predicted_tags.update([tag.lower() for tag in result['recommended_tags']])\n",
        "        # Add ground truth tags\n",
        "        all_ground_truth_tags.update([tag.lower() for tag in result['ground_truth_tags']])\n",
        "\n",
        "# Combine all unique tags\n",
        "all_unique_tags = list(all_predicted_tags.union(all_ground_truth_tags))\n",
        "print(f\"Total unique predicted tags: {len(all_predicted_tags)}\")\n",
        "print(f\"Total unique ground truth tags: {len(all_ground_truth_tags)}\")\n",
        "print(f\"Total unique tags to encode: {len(all_unique_tags)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode all tags with SBERT\n",
        "print(\"Encoding all tags with SBERT...\")\n",
        "tag_sbert_embeddings = sbert_model.encode(all_unique_tags, show_progress_bar=True)\n",
        "print(f\"SBERT embeddings shape: {tag_sbert_embeddings.shape}\")\n",
        "\n",
        "# Create tag to embedding mapping\n",
        "tag_to_sbert = {tag: embedding for tag, embedding in zip(all_unique_tags, tag_sbert_embeddings)}\n",
        "print(\"SBERT encoding completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Semantic Similarity Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_semantic_hits(predicted_tags, ground_truth_tags, tag_to_sbert, similarity_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Compute semantic hits using SBERT embeddings and cosine similarity.\n",
        "    A predicted tag is considered a hit if its semantic similarity\n",
        "    with any ground truth tag exceeds the threshold.\n",
        "    \"\"\"\n",
        "    hits = []\n",
        "    semantic_matches = []\n",
        "    \n",
        "    predicted_tags_lower = [tag.lower() for tag in predicted_tags]\n",
        "    ground_truth_tags_lower = [tag.lower() for tag in ground_truth_tags]\n",
        "    \n",
        "    for pred_tag in predicted_tags_lower:\n",
        "        if pred_tag not in tag_to_sbert:\n",
        "            continue\n",
        "            \n",
        "        pred_embedding = tag_to_sbert[pred_tag]\n",
        "        max_similarity = 0.0\n",
        "        best_match = None\n",
        "        \n",
        "        for gt_tag in ground_truth_tags_lower:\n",
        "            if gt_tag not in tag_to_sbert:\n",
        "                continue\n",
        "                \n",
        "            gt_embedding = tag_to_sbert[gt_tag]\n",
        "            similarity = cosine_similarity([pred_embedding], [gt_embedding])[0][0]\n",
        "            \n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_match = gt_tag\n",
        "        \n",
        "        if max_similarity >= similarity_threshold:\n",
        "            hits.append(pred_tag)\n",
        "            semantic_matches.append({\n",
        "                'predicted': pred_tag,\n",
        "                'matched_gt': best_match,\n",
        "                'similarity': max_similarity\n",
        "            })\n",
        "    \n",
        "    return hits, semantic_matches\n",
        "\n",
        "# Test the semantic similarity function\n",
        "test_pred = ['percussion', 'beat', 'rhythm']\n",
        "test_gt = ['drum', 'drums', 'drumming']\n",
        "test_hits, test_matches = compute_semantic_hits(test_pred, test_gt, tag_to_sbert, SIMILARITY_THRESHOLD)\n",
        "print(f\"Test semantic hits: {test_hits}\")\n",
        "print(f\"Test matches: {test_matches}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Apply SBERT Evaluation to Rankst Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply semantic evaluation to all rankst results\n",
        "print(\"Applying SBERT semantic evaluation to rankst results...\")\n",
        "\n",
        "rankst_sbert_results = {}\n",
        "\n",
        "for k in sorted(rankst_results.keys()):\n",
        "    print(f\"\\nProcessing k={k}...\")\n",
        "    \n",
        "    original_results = rankst_results[k]['evaluation_details']\n",
        "    enhanced_results = []\n",
        "    \n",
        "    total_semantic_hits = 0\n",
        "    total_exact_hits = 0\n",
        "    \n",
        "    for result in tqdm(original_results, desc=f\"Processing k={k}\"):\n",
        "        # Get original data\n",
        "        predicted_tags = result['recommended_tags'][:10]  # Top 10 predictions\n",
        "        ground_truth_tags = result['ground_truth_tags']\n",
        "        exact_hits = result['hits']\n",
        "        \n",
        "        # Calculate semantic hits using SBERT\n",
        "        semantic_hits, semantic_matches = compute_semantic_hits(\n",
        "            predicted_tags, ground_truth_tags, tag_to_sbert, SIMILARITY_THRESHOLD\n",
        "        )\n",
        "        \n",
        "        # Calculate semantic metrics\n",
        "        semantic_precision = len(semantic_hits) / 10.0  # Always 10 predictions\n",
        "        semantic_recall = len(semantic_hits) / len(ground_truth_tags) if ground_truth_tags else 0.0\n",
        "        semantic_f1 = 2 * (semantic_precision * semantic_recall) / (semantic_precision + semantic_recall) if (semantic_precision + semantic_recall) > 0 else 0.0\n",
        "        \n",
        "        # Create enhanced result\n",
        "        enhanced_result = result.copy()  # Copy original result\n",
        "        enhanced_result.update({\n",
        "            'semantic_hits': semantic_hits,\n",
        "            'semantic_matches': semantic_matches,\n",
        "            'num_semantic_hits': len(semantic_hits),\n",
        "            'num_exact_hits': len(exact_hits),\n",
        "            'semantic_precision_10': semantic_precision,\n",
        "            'semantic_recall_10': semantic_recall,\n",
        "            'semantic_f1_10': semantic_f1,\n",
        "            # Keep original metrics for comparison\n",
        "            'exact_precision_10': result['precision_10'],\n",
        "            'exact_recall_10': result['recall_10'],\n",
        "            'exact_f1_10': result['f1_10']\n",
        "        })\n",
        "        \n",
        "        enhanced_results.append(enhanced_result)\n",
        "        total_semantic_hits += len(semantic_hits)\n",
        "        total_exact_hits += len(exact_hits)\n",
        "    \n",
        "    improvement = (total_semantic_hits / total_exact_hits - 1) * 100 if total_exact_hits > 0 else 0\n",
        "    print(f\"k={k}: {total_semantic_hits} semantic hits vs {total_exact_hits} exact hits ({improvement:.1f}% improvement)\")\n",
        "    \n",
        "    # Store enhanced results (we'll compute metrics in next cell)\n",
        "    rankst_sbert_results[k] = {\n",
        "        'enhanced_results': enhanced_results,\n",
        "        'total_semantic_hits': total_semantic_hits,\n",
        "        'total_exact_hits': total_exact_hits\n",
        "    }\n",
        "\n",
        "print(\"\\nSBERT evaluation completed for all k values!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute final metrics for each k\n",
        "for k in sorted(rankst_sbert_results.keys()):\n",
        "    enhanced_results = rankst_sbert_results[k]['enhanced_results']\n",
        "    \n",
        "    # Calculate overall metrics for this k\n",
        "    semantic_precisions = [r['semantic_precision_10'] for r in enhanced_results]\n",
        "    semantic_recalls = [r['semantic_recall_10'] for r in enhanced_results]\n",
        "    semantic_f1s = [r['semantic_f1_10'] for r in enhanced_results]\n",
        "    \n",
        "    exact_precisions = [r['exact_precision_10'] for r in enhanced_results]\n",
        "    exact_recalls = [r['exact_recall_10'] for r in enhanced_results]\n",
        "    exact_f1s = [r['exact_f1_10'] for r in enhanced_results]\n",
        "    \n",
        "    # Update results structure with complete metrics\n",
        "    rankst_sbert_results[k].update({\n",
        "        'k_input_tags': k,\n",
        "        'sbert_threshold': SIMILARITY_THRESHOLD,\n",
        "        'sbert_model': 'all-MiniLM-L6-v2',\n",
        "        'total_test_cases': len(enhanced_results),\n",
        "        'semantic_metrics': {\n",
        "            'precision_10': {\n",
        "                'mean': np.mean(semantic_precisions),\n",
        "                'median': np.median(semantic_precisions),\n",
        "                'std': np.std(semantic_precisions),\n",
        "                'min': np.min(semantic_precisions),\n",
        "                'max': np.max(semantic_precisions)\n",
        "            },\n",
        "            'recall_10': {\n",
        "                'mean': np.mean(semantic_recalls),\n",
        "                'median': np.median(semantic_recalls),\n",
        "                'std': np.std(semantic_recalls),\n",
        "                'min': np.min(semantic_recalls),\n",
        "                'max': np.max(semantic_recalls)\n",
        "            },\n",
        "            'f1_10': {\n",
        "                'mean': np.mean(semantic_f1s),\n",
        "                'median': np.median(semantic_f1s),\n",
        "                'std': np.std(semantic_f1s),\n",
        "                'min': np.min(semantic_f1s),\n",
        "                'max': np.max(semantic_f1s)\n",
        "            }\n",
        "        },\n",
        "        'exact_metrics': {\n",
        "            'precision_10': {\n",
        "                'mean': np.mean(exact_precisions),\n",
        "                'median': np.median(exact_precisions),\n",
        "                'std': np.std(exact_precisions),\n",
        "                'min': np.min(exact_precisions),\n",
        "                'max': np.max(exact_precisions)\n",
        "            },\n",
        "            'recall_10': {\n",
        "                'mean': np.mean(exact_recalls),\n",
        "                'median': np.median(exact_recalls),\n",
        "                'std': np.std(exact_recalls),\n",
        "                'min': np.min(exact_recalls),\n",
        "                'max': np.max(exact_recalls)\n",
        "            },\n",
        "            'f1_10': {\n",
        "                'mean': np.mean(exact_f1s),\n",
        "                'median': np.median(exact_f1s),\n",
        "                'std': np.std(exact_f1s),\n",
        "                'min': np.min(exact_f1s),\n",
        "                'max': np.max(exact_f1s)\n",
        "            }\n",
        "        },\n",
        "        'evaluation_details': enhanced_results\n",
        "    })\n",
        "\n",
        "print(\"Metrics computation completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Results Analysis and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display comparison of exact vs semantic metrics\n",
        "print(\"=== Rankst Results: Exact vs Semantic Matching Comparison ===\")\n",
        "print(f\"SBERT Threshold: {SIMILARITY_THRESHOLD}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRECISION@10\")\n",
        "print(\"=\"*80)\n",
        "print(\"k | Exact      | Semantic   | Improvement\")\n",
        "print(\"--|------------|------------|------------\")\n",
        "for k in sorted(rankst_sbert_results.keys()):\n",
        "    exact_p = rankst_sbert_results[k]['exact_metrics']['precision_10']['mean']\n",
        "    semantic_p = rankst_sbert_results[k]['semantic_metrics']['precision_10']['mean']\n",
        "    improvement = (semantic_p / exact_p - 1) * 100 if exact_p > 0 else 0\n",
        "    print(f\"{k} | {exact_p:.4f}     | {semantic_p:.4f}     | +{improvement:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RECALL@10\")\n",
        "print(\"=\"*80)\n",
        "print(\"k | Exact      | Semantic   | Improvement\")\n",
        "print(\"--|------------|------------|------------\")\n",
        "for k in sorted(rankst_sbert_results.keys()):\n",
        "    exact_r = rankst_sbert_results[k]['exact_metrics']['recall_10']['mean']\n",
        "    semantic_r = rankst_sbert_results[k]['semantic_metrics']['recall_10']['mean']\n",
        "    improvement = (semantic_r / exact_r - 1) * 100 if exact_r > 0 else 0\n",
        "    print(f\"{k} | {exact_r:.4f}     | {semantic_r:.4f}     | +{improvement:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"F1@10\")\n",
        "print(\"=\"*80)\n",
        "print(\"k | Exact      | Semantic   | Improvement\")\n",
        "print(\"--|------------|------------|------------\")\n",
        "for k in sorted(rankst_sbert_results.keys()):\n",
        "    exact_f = rankst_sbert_results[k]['exact_metrics']['f1_10']['mean']\n",
        "    semantic_f = rankst_sbert_results[k]['semantic_metrics']['f1_10']['mean']\n",
        "    improvement = (semantic_f / exact_f - 1) * 100 if exact_f > 0 else 0\n",
        "    print(f\"{k} | {exact_f:.4f}     | {semantic_f:.4f}     | +{improvement:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOTAL HITS\")\n",
        "print(\"=\"*80)\n",
        "print(\"k | Exact Hits | Semantic Hits | Improvement\")\n",
        "print(\"--|------------|---------------|------------\")\n",
        "for k in sorted(rankst_sbert_results.keys()):\n",
        "    exact_hits = rankst_sbert_results[k]['total_exact_hits']\n",
        "    semantic_hits = rankst_sbert_results[k]['total_semantic_hits']\n",
        "    improvement = (semantic_hits / exact_hits - 1) * 100 if exact_hits > 0 else 0\n",
        "    print(f\"{k} | {exact_hits:10d} | {semantic_hits:13d} | +{improvement:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show example results with semantic analysis\n",
        "print(\"=== Example Results with SBERT Semantic Analysis ===\")\n",
        "\n",
        "# Find best examples for each k (highest semantic hits)\n",
        "for k in sorted(rankst_sbert_results.keys()):\n",
        "    results_k = rankst_sbert_results[k]['evaluation_details']\n",
        "    # Sort by number of semantic hits\n",
        "    best_results = sorted(results_k, key=lambda x: x['num_semantic_hits'], reverse=True)[:2]\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BEST EXAMPLES FOR k={k}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    for i, result in enumerate(best_results, 1):\n",
        "        print(f\"\\n--- Example {i} (Sound ID: {result['sound_id']}) ---\")\n",
        "        print(f\"Title: {result['title']}\")\n",
        "        print(f\"Input tags (k={k}): {result['input_tags_k']}\")\n",
        "        print(f\"Ground truth tags: {result['ground_truth_tags']}\")\n",
        "        print(f\"Predicted tags (top 10): {result['recommended_tags'][:10]}\")\n",
        "        \n",
        "        print(f\"\\nSemantic Matches (threshold={SIMILARITY_THRESHOLD}):\")\n",
        "        for match in result['semantic_matches']:\n",
        "            print(f\"  '{match['predicted']}' â†” '{match['matched_gt']}' (similarity: {match['similarity']:.3f})\")\n",
        "        \n",
        "        print(f\"\\nResults:\")\n",
        "        print(f\"  Exact hits: {result['hits']} ({result['num_exact_hits']} hits)\")\n",
        "        print(f\"  Semantic hits: {result['semantic_hits']} ({result['num_semantic_hits']} hits)\")\n",
        "        print(f\"  Exact    - P@10: {result['exact_precision_10']:.3f}, R@10: {result['exact_recall_10']:.3f}, F1@10: {result['exact_f1_10']:.3f}\")\n",
        "        print(f\"  Semantic - P@10: {result['semantic_precision_10']:.3f}, R@10: {result['semantic_recall_10']:.3f}, F1@10: {result['semantic_f1_10']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze hit distributions\n",
        "print(\"\\n=== Hit Distribution Analysis ===\")\n",
        "\n",
        "for k in sorted(rankst_sbert_results.keys()):\n",
        "    results_k = rankst_sbert_results[k]['evaluation_details']\n",
        "    \n",
        "    # Count hit distributions\n",
        "    exact_hit_counts = {}\n",
        "    semantic_hit_counts = {}\n",
        "    \n",
        "    for result in results_k:\n",
        "        exact_hits = result['num_exact_hits']\n",
        "        semantic_hits = result['num_semantic_hits']\n",
        "        \n",
        "        exact_hit_counts[exact_hits] = exact_hit_counts.get(exact_hits, 0) + 1\n",
        "        semantic_hit_counts[semantic_hits] = semantic_hit_counts.get(semantic_hits, 0) + 1\n",
        "    \n",
        "    print(f\"\\nk={k} Hit Distribution:\")\n",
        "    print(\"Hits | Exact Count | Semantic Count\")\n",
        "    print(\"-----|-------------|---------------\")\n",
        "    \n",
        "    max_hits = max(max(exact_hit_counts.keys()), max(semantic_hit_counts.keys()))\n",
        "    for hits in range(max_hits + 1):\n",
        "        exact_count = exact_hit_counts.get(hits, 0)\n",
        "        semantic_count = semantic_hit_counts.get(hits, 0)\n",
        "        print(f\"{hits:4d} | {exact_count:11d} | {semantic_count:14d}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert numpy values to Python types for JSON serialization\n",
        "def convert_numpy_types(obj):\n",
        "    \"\"\"Recursively convert numpy types to Python types\"\"\"\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_types(item) for item in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Convert results to be JSON serializable\n",
        "json_safe_results = convert_numpy_types(rankst_sbert_results)\n",
        "\n",
        "# Save detailed results for each k\n",
        "os.makedirs('eval', exist_ok=True)\n",
        "\n",
        "for k in sorted(rankst_sbert_results.keys()):\n",
        "    # Save detailed results\n",
        "    output_file = f'eval/rankst_k{k}_sbert_threshold{SIMILARITY_THRESHOLD}_results.json'\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(json_safe_results[k], f, indent=2)\n",
        "    print(f\"Detailed results for k={k} saved to {output_file}\")\n",
        "    \n",
        "    # Save summary only\n",
        "    summary_k = {key: json_safe_results[k][key] for key in json_safe_results[k] if key != 'evaluation_details'}\n",
        "    summary_file = f'eval/rankst_k{k}_sbert_threshold{SIMILARITY_THRESHOLD}_summary.json'\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary_k, f, indent=2)\n",
        "    print(f\"Summary for k={k} saved to {summary_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create combined summary comparing all k values\n",
        "combined_summary = {\n",
        "    'description': f'Rankst tag recommendation evaluation with SBERT semantic similarity (threshold={SIMILARITY_THRESHOLD})',\n",
        "    'sbert_model': 'all-MiniLM-L6-v2',\n",
        "    'sbert_threshold': float(SIMILARITY_THRESHOLD),\n",
        "    'total_test_cases': rankst_sbert_results[1]['total_test_cases'],\n",
        "    'k_values_tested': sorted(list(rankst_sbert_results.keys())),\n",
        "    'semantic_results_by_k': {},\n",
        "    'exact_results_by_k': {},\n",
        "    'improvements_by_k': {}\n",
        "}\n",
        "\n",
        "for k in sorted(rankst_sbert_results.keys()):\n",
        "    k_str = str(k)\n",
        "    \n",
        "    # Semantic results\n",
        "    combined_summary['semantic_results_by_k'][k_str] = convert_numpy_types(\n",
        "        rankst_sbert_results[k]['semantic_metrics']\n",
        "    )\n",
        "    \n",
        "    # Exact results (for comparison)\n",
        "    combined_summary['exact_results_by_k'][k_str] = convert_numpy_types(\n",
        "        rankst_sbert_results[k]['exact_metrics']\n",
        "    )\n",
        "    \n",
        "    # Improvements\n",
        "    exact_metrics = rankst_sbert_results[k]['exact_metrics']\n",
        "    semantic_metrics = rankst_sbert_results[k]['semantic_metrics']\n",
        "    \n",
        "    improvements = {}\n",
        "    for metric in ['precision_10', 'recall_10', 'f1_10']:\n",
        "        exact_val = exact_metrics[metric]['mean']\n",
        "        semantic_val = semantic_metrics[metric]['mean']\n",
        "        improvement = (semantic_val / exact_val - 1) * 100 if exact_val > 0 else 0\n",
        "        improvements[metric + '_improvement_percent'] = float(improvement)\n",
        "    \n",
        "    # Hit count improvements\n",
        "    exact_hits = rankst_sbert_results[k]['total_exact_hits']\n",
        "    semantic_hits = rankst_sbert_results[k]['total_semantic_hits']\n",
        "    hit_improvement = (semantic_hits / exact_hits - 1) * 100 if exact_hits > 0 else 0\n",
        "    improvements['total_hits_improvement_percent'] = float(hit_improvement)\n",
        "    \n",
        "    combined_summary['improvements_by_k'][k_str] = improvements\n",
        "\n",
        "# Save combined summary\n",
        "combined_file = f'eval/rankst_k_sbert_threshold{SIMILARITY_THRESHOLD}_combined_summary.json'\n",
        "with open(combined_file, 'w') as f:\n",
        "    json.dump(combined_summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nCombined summary saved to {combined_file}\")\n",
        "print(\"\\n=== SBERT Evaluation Complete! ===\")\n",
        "print(f\"Results saved for k values: {sorted(list(rankst_sbert_results.keys()))}\")\n",
        "print(f\"SBERT threshold used: {SIMILARITY_THRESHOLD}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "clap2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
