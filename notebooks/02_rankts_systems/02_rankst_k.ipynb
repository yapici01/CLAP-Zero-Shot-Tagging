{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Tag Recommendation System - Ranking Evaluation with Variable Input Tags (k=1,2,3)\n",
        "\n",
        "This notebook evaluates the tagrecommendation-k10 system by:\n",
        "\n",
        "1. Loading the input/ground truth pairs from data preparation\n",
        "2. Testing API connection with sample cases\n",
        "3. Running evaluation with k=1, 2, and 3 input tags\n",
        "4. For each k, creating subsets of the original 3 input tags\n",
        "5. Calculating precision, recall, and F1 metrics at 10 for each k\n",
        "6. Saving evaluation results with separate files for each k\n",
        "\n",
        "## Evaluation Metrics:\n",
        "- **Precision@10**: How many of the top 10 recommended tags are in ground truth\n",
        "- **Recall@10**: How many ground truth tags are found in top 10 recommendations  \n",
        "- **F1@10**: Harmonic mean of precision and recall\n",
        "\n",
        "## Variable Input Tags:\n",
        "- **k=1**: Use only 1 input tag (first tag from original 3)\n",
        "- **k=2**: Use 2 input tags (first 2 tags from original 3) \n",
        "- **k=3**: Use all 3 input tags (original setup)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import pickle\n",
        "import json\n",
        "from typing import List, Dict\n",
        "import itertools\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the input/ground truth pairs\n",
        "print(\"Loading input/ground truth pairs...\")\n",
        "with open('data/input_ground_truth_pairs.pkl', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded {len(test_data)} test cases\")\n",
        "print(f\"Average ground truth tags per sound: {np.mean([len(item['ground_truth_tags']) for item in test_data]):.2f}\")\n",
        "\n",
        "# Analyze input tag distribution\n",
        "input_tag_counts = [len(item['input_tags']) for item in test_data]\n",
        "print(f\"Input tags distribution:\")\n",
        "print(f\"  Min tags: {min(input_tag_counts)}\")\n",
        "print(f\"  Max tags: {max(input_tag_counts)}\")\n",
        "print(f\"  Average tags: {np.mean(input_tag_counts):.2f}\")\n",
        "\n",
        "# Filter to only cases with at least 3 input tags for fair comparison\n",
        "test_data_3plus = [item for item in test_data if len(item['input_tags']) >= 3]\n",
        "print(f\"\\nFiltered to {len(test_data_3plus)} cases with 3+ input tags\")\n",
        "\n",
        "# Show first test case as example\n",
        "print(f\"\\nExample test case:\")\n",
        "print(f\"Sound ID: {test_data_3plus[0]['sound_id']}\")\n",
        "print(f\"Title: {test_data_3plus[0]['title']}\")\n",
        "print(f\"Input tags: {test_data_3plus[0]['input_tags']}\")\n",
        "print(f\"Ground truth tags: {test_data_3plus[0]['ground_truth_tags']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_freesound_recommendations_k10(input_tags, host=\"http://localhost:8011\"):\n",
        "    \"\"\"\n",
        "    Call the tagrecommendation system API\n",
        "    \"\"\"\n",
        "    url = f\"{host}/tagrecommendation/recommend_tags/\"\n",
        "    params = {\"input_tags\": \",\".join(input_tags)}\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(url, params=params, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return response.json().get(\"result\", {}).get(\"tags\", [])\n",
        "        else:\n",
        "            print(f\"Error {response.status_code}: {response.text}\")\n",
        "            return []\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "        return []\n",
        "\n",
        "# Test the API with a sample case first\n",
        "print(\"Testing API connection...\")\n",
        "sample_case = test_data_3plus[0]\n",
        "print(f\"Sample input tags (all 3): {sample_case['input_tags'][:3]}\")\n",
        "\n",
        "recommended_tags = get_freesound_recommendations_k10(sample_case['input_tags'][:3])\n",
        "print(f\"Recommended tags: {recommended_tags}\")\n",
        "print(f\"Ground truth tags: {sample_case['ground_truth_tags']}\")\n",
        "\n",
        "if recommended_tags:\n",
        "    print(\"✅ API connection successful!\")\n",
        "else:\n",
        "    print(\"❌ API connection failed. Please check if the server is running on localhost:8011\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_input_tag_subsets(input_tags, k):\n",
        "    \"\"\"\n",
        "    Generate k-sized subsets of input tags\n",
        "    For k=1,2: take first k tags to ensure consistency\n",
        "    For k=3: take first 3 tags\n",
        "    \"\"\"\n",
        "    if k <= len(input_tags):\n",
        "        return input_tags[:k]\n",
        "    else:\n",
        "        return input_tags  # If fewer than k tags available, use all\n",
        "\n",
        "def evaluate_recommendations_k(test_data: List[Dict], k: int, max_tests: int = None):\n",
        "    \"\"\"\n",
        "    Evaluate the tagrecommendation system on test data with k input tags\n",
        "    \"\"\"\n",
        "    if max_tests is None:\n",
        "        max_tests = len(test_data)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(f\"Evaluating with k={k} input tags on {min(len(test_data), max_tests)} test cases...\")\n",
        "    \n",
        "    for i, case in enumerate(test_data[:max_tests]):\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Progress: {i}/{min(len(test_data), max_tests)}\")\n",
        "        \n",
        "        # Generate k-sized subset of input tags\n",
        "        input_tags_k = generate_input_tag_subsets(case['input_tags'], k)\n",
        "        ground_truth = set(case['ground_truth_tags'])\n",
        "        \n",
        "        # Get recommendations\n",
        "        recommended_tags = get_freesound_recommendations_k10(input_tags_k)\n",
        "        \n",
        "        if recommended_tags:\n",
        "            # Calculate metrics\n",
        "            recommended_set = set(recommended_tags[:10])  # Top 10 recommendations\n",
        "            \n",
        "            # Precision@10: How many recommended tags are in ground truth\n",
        "            precision_10 = len(recommended_set.intersection(ground_truth)) / len(recommended_set) if recommended_set else 0\n",
        "            \n",
        "            # Recall@10: How many ground truth tags are found in recommendations\n",
        "            recall_10 = len(recommended_set.intersection(ground_truth)) / len(ground_truth) if ground_truth else 0\n",
        "            \n",
        "            # F1@10: Harmonic mean of precision and recall\n",
        "            if precision_10 + recall_10 > 0:\n",
        "                f1_10 = 2 * (precision_10 * recall_10) / (precision_10 + recall_10)\n",
        "            else:\n",
        "                f1_10 = 0.0\n",
        "            \n",
        "            results.append({\n",
        "                'sound_id': case['sound_id'],\n",
        "                'title': case['title'],\n",
        "                'k': k,\n",
        "                'original_input_tags': case['input_tags'],\n",
        "                'input_tags_k': input_tags_k,\n",
        "                'recommended_tags': recommended_tags[:10],\n",
        "                'ground_truth_tags': list(ground_truth),\n",
        "                'precision_10': precision_10,\n",
        "                'recall_10': recall_10,\n",
        "                'f1_10': f1_10,\n",
        "                'hits': list(recommended_set.intersection(ground_truth))\n",
        "            })\n",
        "        else:\n",
        "            # If API call failed, record zeros\n",
        "            results.append({\n",
        "                'sound_id': case['sound_id'],\n",
        "                'title': case['title'],\n",
        "                'k': k,\n",
        "                'original_input_tags': case['input_tags'],\n",
        "                'input_tags_k': input_tags_k,\n",
        "                'recommended_tags': [],\n",
        "                'ground_truth_tags': list(ground_truth),\n",
        "                'precision_10': 0.0,\n",
        "                'recall_10': 0.0,\n",
        "                'f1_10': 0.0,\n",
        "                'hits': []\n",
        "            })\n",
        "        \n",
        "        # Small delay to avoid overwhelming the API\n",
        "        time.sleep(0.1)\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Evaluation function defined. Ready to run evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation for k=1, 2, 3\n",
        "k_values = [1, 2, 3]\n",
        "all_results = {}\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Starting evaluation for k={k}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    evaluation_results = evaluate_recommendations_k(test_data_3plus, k)\n",
        "    all_results[k] = evaluation_results\n",
        "    \n",
        "    print(f\"\\nEvaluation for k={k} complete! Processed {len(evaluation_results)} cases.\")\n",
        "\n",
        "print(f\"\\n🎉 All evaluations complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze evaluation results for each k\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_stats_all = {}\n",
        "\n",
        "for k in k_values:\n",
        "    if k in all_results and all_results[k]:\n",
        "        results = all_results[k]\n",
        "        precisions = [r['precision_10'] for r in results]\n",
        "        recalls = [r['recall_10'] for r in results]\n",
        "        f1_scores = [r['f1_10'] for r in results]\n",
        "        \n",
        "        print(f\"\\nResults for k={k} input tags:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Average Precision@10: {np.mean(precisions):.3f}\")\n",
        "        print(f\"Average Recall@10: {np.mean(recalls):.3f}\")\n",
        "        print(f\"Average F1@10: {np.mean(f1_scores):.3f}\")\n",
        "        print(f\"Median Precision@10: {np.median(precisions):.3f}\")\n",
        "        print(f\"Median Recall@10: {np.median(recalls):.3f}\")\n",
        "        print(f\"Median F1@10: {np.median(f1_scores):.3f}\")\n",
        "        print(f\"Std Precision@10: {np.std(precisions):.3f}\")\n",
        "        print(f\"Std Recall@10: {np.std(recalls):.3f}\")\n",
        "        print(f\"Std F1@10: {np.std(f1_scores):.3f}\")\n",
        "        \n",
        "        # Store summary stats\n",
        "        summary_stats_all[k] = {\n",
        "            'precision_10': {\n",
        "                'mean': float(np.mean(precisions)),\n",
        "                'median': float(np.median(precisions)),\n",
        "                'std': float(np.std(precisions)),\n",
        "                'min': float(np.min(precisions)),\n",
        "                'max': float(np.max(precisions))\n",
        "            },\n",
        "            'recall_10': {\n",
        "                'mean': float(np.mean(recalls)),\n",
        "                'median': float(np.median(recalls)),\n",
        "                'std': float(np.std(recalls)),\n",
        "                'min': float(np.min(recalls)),\n",
        "                'max': float(np.max(recalls))\n",
        "            },\n",
        "            'f1_10': {\n",
        "                'mean': float(np.mean(f1_scores)),\n",
        "                'median': float(np.median(f1_scores)),\n",
        "                'std': float(np.std(f1_scores)),\n",
        "                'min': float(np.min(f1_scores)),\n",
        "                'max': float(np.max(f1_scores))\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Comparison table\n",
        "print(f\"\\n\\nCOMPARISON ACROSS k VALUES:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<15} {'k=1':<10} {'k=2':<10} {'k=3':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for metric in ['precision_10', 'recall_10', 'f1_10']:\n",
        "    values = []\n",
        "    for k in k_values:\n",
        "        if k in summary_stats_all:\n",
        "            values.append(f\"{summary_stats_all[k][metric]['mean']:.3f}\")\n",
        "        else:\n",
        "            values.append(\"N/A\")\n",
        "    print(f\"{metric.replace('_10', '@10'):<15} {values[0]:<10} {values[1]:<10} {values[2]:<10}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show example results for each k\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXAMPLE RESULTS FOR EACH k\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Use the same test case for all k values to see the effect\n",
        "example_idx = 0\n",
        "\n",
        "for k in k_values:\n",
        "    if k in all_results and all_results[k]:\n",
        "        result = all_results[k][example_idx]\n",
        "        print(f\"\\nk={k} - Sound ID: {result['sound_id']}\")\n",
        "        print(f\"  Title: {result['title']}\")\n",
        "        print(f\"  Original input tags: {result['original_input_tags']}\")\n",
        "        print(f\"  Input tags used (k={k}): {result['input_tags_k']}\")\n",
        "        print(f\"  Recommended: {result['recommended_tags']}\")\n",
        "        print(f\"  Ground Truth: {result['ground_truth_tags']}\")\n",
        "        print(f\"  Hits: {result['hits']}\")\n",
        "        print(f\"  Precision@10: {result['precision_10']:.3f}\")\n",
        "        print(f\"  Recall@10: {result['recall_10']:.3f}\")\n",
        "        print(f\"  F1@10: {result['f1_10']:.3f}\")\n",
        "        print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save evaluation results for each k\n",
        "print(\"\\nSaving evaluation results...\")\n",
        "\n",
        "for k in k_values:\n",
        "    if k in all_results and all_results[k]:\n",
        "        results = all_results[k]\n",
        "        \n",
        "        # Prepare summary data for this k\n",
        "        summary_data = {\n",
        "            'k_input_tags': k,\n",
        "            'total_test_cases': len(results),\n",
        "            'metrics': summary_stats_all[k],\n",
        "            'evaluation_details': results\n",
        "        }\n",
        "        \n",
        "        # Save detailed results\n",
        "        output_file = f'eval/rankst_k{k}_eval.json'\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(summary_data, f, indent=2)\n",
        "        \n",
        "        print(f\"✅ k={k} results saved to {output_file}\")\n",
        "        \n",
        "        # Save compact summary\n",
        "        compact_summary = {\n",
        "            'k_input_tags': k,\n",
        "            'total_test_cases': len(results),\n",
        "            'metrics': summary_stats_all[k]\n",
        "        }\n",
        "        \n",
        "        compact_file = f'eval/rankst_k{k}_eval_summary.json'\n",
        "        with open(compact_file, 'w') as f:\n",
        "            json.dump(compact_summary, f, indent=2)\n",
        "        \n",
        "        print(f\"✅ k={k} summary saved to {compact_file}\")\n",
        "\n",
        "# Save combined summary across all k values\n",
        "combined_summary = {\n",
        "    'description': 'Tag recommendation evaluation with varying input tag counts (k=1,2,3)',\n",
        "    'total_test_cases': len(test_data_3plus),\n",
        "    'k_values_tested': k_values,\n",
        "    'results_by_k': summary_stats_all\n",
        "}\n",
        "\n",
        "combined_file = 'eval/rankst_k_combined_summary.json'\n",
        "with open(combined_file, 'w') as f:\n",
        "    json.dump(combined_summary, f, indent=2)\n",
        "\n",
        "print(f\"✅ Combined summary saved to {combined_file}\")\n",
        "print(\"\\n🎉 All evaluation results saved successfully!\")\n",
        "\n",
        "# Print final summary\n",
        "print(f\"\\nFINAL SUMMARY:\")\n",
        "print(f\"- Evaluated {len(test_data_3plus)} test cases\")\n",
        "print(f\"- Tested with k={k_values} input tags\")\n",
        "print(f\"- Results saved to eval/ directory with k-specific filenames\")\n",
        "print(f\"- No conflicts with previous rankst_eval.json files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
