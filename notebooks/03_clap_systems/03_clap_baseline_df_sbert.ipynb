{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# CLAP Zero-Shot Tag Recommendation System with DF Weighting and SBERT Semantic Evaluation\n",
        "\n",
        "This notebook implements a zero-shot tag recommendation system using LAION-CLAP embeddings with Document Frequency (DF) weighting.\n",
        "It loads pre-computed tag embeddings and modifies the similarity scoring to include normalized DF-based weighting with a tunable alpha parameter.\n",
        "The final similarity score is computed as: cosine_similarity Ã— ((1 - alpha) + alpha Ã— (log(1 + df) / log(1 + N)))\n",
        "\n",
        "**Key difference**: Uses SBERT (Sentence-BERT) for semantic similarity evaluation instead of exact string matching for hit detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import laion_clap\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from collections import Counter\n",
        "import math\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load input/ground truth pairs\n",
        "with open('data/input_ground_truth_pairs.json', 'r') as f:\n",
        "    input_gt_pairs = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(input_gt_pairs)} sound clips with input/ground truth pairs\")\n",
        "print(f\"Example entry: {input_gt_pairs[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metadata\n",
        "metadata_df = pd.read_csv('data/BSD10k/BSD10K_metadata_filtered.csv')\n",
        "print(f\"Loaded metadata for {len(metadata_df)} sound clips\")\n",
        "print(metadata_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tagset\n",
        "with open('data/tagset_clap_normalized_hyphen_unique.txt', 'r') as f:\n",
        "    clap_tags = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Loaded {len(clap_tags)} unique CLAP tags\")\n",
        "print(f\"First 10 tags: {clap_tags[:10]}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Compute Document Frequency (DF) Scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute document frequency from metadata\n",
        "print(\"Computing document frequency scores...\")\n",
        "\n",
        "# Count tag occurrences across all documents\n",
        "tag_document_counts = Counter()\n",
        "total_documents = len(metadata_df)\n",
        "\n",
        "for _, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=\"Processing documents\"):\n",
        "    tags = row['tags'].split(',')\n",
        "    tags = [tag.strip().lower() for tag in tags]  # Normalize to lowercase\n",
        "    \n",
        "    # Count unique tags in this document\n",
        "    unique_tags = set(tags)\n",
        "    for tag in unique_tags:\n",
        "        tag_document_counts[tag] += 1\n",
        "\n",
        "print(f\"Found {len(tag_document_counts)} unique tags in the dataset\")\n",
        "print(f\"Total documents: {total_documents}\")\n",
        "\n",
        "# Show top 10 most frequent tags\n",
        "print(\"\\nTop 10 most frequent tags:\")\n",
        "for tag, count in tag_document_counts.most_common(10):\n",
        "    percentage = (count / total_documents) * 100\n",
        "    print(f\"{tag}: {count} documents ({percentage:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DF mapping for CLAP tags\n",
        "print(\"Creating DF mapping for CLAP tags...\")\n",
        "\n",
        "# Alpha parameter for DF weighting (tunable)\n",
        "ALPHA = 0.7\n",
        "\n",
        "# Create DF scores for each CLAP tag\n",
        "tag_df_scores = {}\n",
        "matched_tags = 0\n",
        "unmatched_tags = 0\n",
        "\n",
        "for tag in clap_tags:\n",
        "    tag_lower = tag.lower()\n",
        "    \n",
        "    if tag_lower in tag_document_counts:\n",
        "        df_count = tag_document_counts[tag_lower]\n",
        "        # Compute normalized DF score with alpha weighting\n",
        "        # Formula: (1 - alpha) + alpha * (log(1 + df) / log(1 + N))\n",
        "        # This interpolates between no DF weighting and normalized DF weighting\n",
        "        normalized_df = math.log(1 + df_count) / math.log(1 + total_documents)\n",
        "        df_score = (1 - ALPHA) + ALPHA * normalized_df\n",
        "        tag_df_scores[tag] = df_score\n",
        "        matched_tags += 1\n",
        "    else:\n",
        "        # For tags not found in dataset, use minimum DF (df=0)\n",
        "        # Formula: (1 - alpha) + alpha * (log(1 + 0) / log(1 + N)) = (1 - alpha)\n",
        "        default_df_score = 1 - ALPHA  # Minimum possible weight\n",
        "        tag_df_scores[tag] = default_df_score\n",
        "        unmatched_tags += 1\n",
        "\n",
        "print(f\"\\nDF Mapping Results:\")\n",
        "print(f\"Matched tags: {matched_tags}\")\n",
        "print(f\"Unmatched tags: {unmatched_tags}\")\n",
        "print(f\"Alpha parameter: {ALPHA}\")\n",
        "\n",
        "# Show DF score statistics\n",
        "df_scores_values = list(tag_df_scores.values())\n",
        "print(f\"\\nDF Score Statistics:\")\n",
        "print(f\"Min DF score: {min(df_scores_values):.4f}\")\n",
        "print(f\"Max DF score: {max(df_scores_values):.4f}\")\n",
        "print(f\"Mean DF score: {np.mean(df_scores_values):.4f}\")\n",
        "print(f\"Std DF score: {np.std(df_scores_values):.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Load Pre-computed Tag Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-computed tag embeddings (hyphen-normalized)\n",
        "print(\"Loading pre-computed tag embeddings...\")\n",
        "\n",
        "tags_embeddings_dir = Path('data/BSD10k/embeddings/tags')\n",
        "embeddings_path = tags_embeddings_dir / 'tag_embeddings_hyphen.npy'\n",
        "\n",
        "# Load embeddings array\n",
        "tag_embeddings_array = np.load(embeddings_path)\n",
        "print(f\"Loaded tag embeddings with shape: {tag_embeddings_array.shape}\")\n",
        "\n",
        "# The embeddings were saved in the same order as clap_tags\n",
        "# So we can use the clap_tags list as the tag names\n",
        "if len(clap_tags) != tag_embeddings_array.shape[0]:\n",
        "    print(f\"WARNING: Mismatch between clap_tags ({len(clap_tags)}) and embeddings ({tag_embeddings_array.shape[0]})\")\n",
        "    # Use the smaller size to avoid index errors\n",
        "    num_tags = min(len(clap_tags), tag_embeddings_array.shape[0])\n",
        "    tag_names_list = clap_tags[:num_tags]\n",
        "    tag_embeddings_array = tag_embeddings_array[:num_tags]\n",
        "else:\n",
        "    tag_names_list = clap_tags\n",
        "    num_tags = len(clap_tags)\n",
        "\n",
        "print(f\"Number of tags: {num_tags}\")\n",
        "print(f\"Embedding dimension: {tag_embeddings_array.shape[2]}\")\n",
        "print(f\"Sentence variants per tag: {tag_embeddings_array.shape[1]}\")\n",
        "\n",
        "# Convert to dictionary format (same as original)\n",
        "tag_embeddings = {}\n",
        "for i, tag in enumerate(tag_names_list):\n",
        "    tag_embeddings[tag] = tag_embeddings_array[i]\n",
        "\n",
        "print(f\"\\nConverted to dictionary format with {len(tag_embeddings)} tags\")\n",
        "print(f\"Example tag embedding shape: {list(tag_embeddings.values())[0].shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Load Audio Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check available audio embeddings\n",
        "embeddings_dir = Path('data/BSD10k/embeddings/clap')\n",
        "available_embeddings = set([int(f.stem) for f in embeddings_dir.glob('*.npy')])\n",
        "\n",
        "print(f\"Found {len(available_embeddings)} audio embeddings\")\n",
        "\n",
        "# Filter input_gt_pairs to only include clips with available embeddings\n",
        "filtered_pairs = [pair for pair in input_gt_pairs if pair['sound_id'] in available_embeddings]\n",
        "\n",
        "print(f\"Using {len(filtered_pairs)} clips with both ground truth and audio embeddings\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load audio embeddings for the filtered clips\n",
        "print(\"Loading audio embeddings...\")\n",
        "\n",
        "audio_embeddings = {}\n",
        "\n",
        "for pair in tqdm(filtered_pairs):\n",
        "    sound_id = pair['sound_id']\n",
        "    embedding_path = embeddings_dir / f\"{sound_id}.npy\"\n",
        "    \n",
        "    if embedding_path.exists():\n",
        "        audio_embeddings[sound_id] = np.load(embedding_path)\n",
        "\n",
        "print(f\"Loaded audio embeddings for {len(audio_embeddings)} sound clips\")\n",
        "if audio_embeddings:\n",
        "    sample_embedding = list(audio_embeddings.values())[0]\n",
        "    print(f\"Audio embedding dimension: {sample_embedding.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Initialize SBERT Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SBERT model for semantic similarity\n",
        "print(\"Loading SBERT model...\")\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"SBERT model loaded successfully!\")\n",
        "\n",
        "# Semantic similarity threshold (adjustable)\n",
        "SIMILARITY_THRESHOLD = 0.7\n",
        "print(f\"Semantic similarity threshold: {SIMILARITY_THRESHOLD}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode all unique tags with SBERT for semantic similarity comparison\n",
        "print(\"Encoding tags with SBERT...\")\n",
        "\n",
        "# Get all unique ground truth tags from the dataset\n",
        "all_gt_tags = set()\n",
        "for pair in filtered_pairs:\n",
        "    all_gt_tags.update([tag.lower() for tag in pair['ground_truth_tags']])\n",
        "\n",
        "# Combine with all CLAP tags\n",
        "all_unique_tags = list(all_gt_tags.union(set([tag.lower() for tag in clap_tags])))\n",
        "print(f\"Total unique tags to encode: {len(all_unique_tags)}\")\n",
        "\n",
        "# Encode all tags\n",
        "tag_sbert_embeddings = sbert_model.encode(all_unique_tags, show_progress_bar=True)\n",
        "print(f\"SBERT embeddings shape: {tag_sbert_embeddings.shape}\")\n",
        "\n",
        "# Create tag to embedding mapping\n",
        "tag_to_sbert = {tag: embedding for tag, embedding in zip(all_unique_tags, tag_sbert_embeddings)}\n",
        "print(\"SBERT encoding completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Compute DF-Weighted Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_tag_recommendations_df_weighted(audio_embedding, tag_embeddings, tag_df_scores, top_k=10):\n",
        "    \"\"\"\n",
        "    Get top-k tag recommendations for a given audio embedding with DF weighting.\n",
        "    For each tag, we take the maximum similarity among its 4 sentence variants,\n",
        "    then multiply by the DF weight.\n",
        "    \n",
        "    Final score = cosine_similarity Ã— df_weight\n",
        "    where df_weight = (1 - alpha) + alpha * (log(1 + df) / log(1 + N))\n",
        "    \"\"\"\n",
        "    tag_similarities = {}\n",
        "    \n",
        "    for tag, text_embeds in tag_embeddings.items():\n",
        "        # Compute cosine similarity between audio and all text variants\n",
        "        similarities = cosine_similarity([audio_embedding], text_embeds)[0]\n",
        "        # Take the maximum similarity among the 4 variants\n",
        "        base_similarity = np.max(similarities)\n",
        "        \n",
        "        # Apply DF weighting\n",
        "        df_weight = tag_df_scores.get(tag, 1.0)  # Default to 1.0 if tag not found\n",
        "        weighted_similarity = base_similarity * df_weight\n",
        "        \n",
        "        tag_similarities[tag] = {\n",
        "            'base_similarity': base_similarity,\n",
        "            'df_weight': df_weight,\n",
        "            'weighted_similarity': weighted_similarity\n",
        "        }\n",
        "    \n",
        "    # Sort tags by weighted similarity and get top-k\n",
        "    sorted_tags = sorted(tag_similarities.items(), key=lambda x: x[1]['weighted_similarity'], reverse=True)\n",
        "    \n",
        "    top_tags = [tag for tag, _ in sorted_tags[:top_k]]\n",
        "    top_scores = [scores['weighted_similarity'] for _, scores in sorted_tags[:top_k]]\n",
        "    top_base_scores = [scores['base_similarity'] for _, scores in sorted_tags[:top_k]]\n",
        "    top_df_weights = [scores['df_weight'] for _, scores in sorted_tags[:top_k]]\n",
        "    \n",
        "    return top_tags, top_scores, top_base_scores, top_df_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_semantic_hits(predicted_tags, ground_truth_tags, tag_to_sbert, similarity_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Compute semantic hits using SBERT embeddings and cosine similarity.\n",
        "    A predicted tag is considered a hit if its semantic similarity\n",
        "    with any ground truth tag exceeds the threshold.\n",
        "    \"\"\"\n",
        "    hits = []\n",
        "    semantic_matches = []\n",
        "    \n",
        "    predicted_tags_lower = [tag.lower() for tag in predicted_tags]\n",
        "    ground_truth_tags_lower = [tag.lower() for tag in ground_truth_tags]\n",
        "    \n",
        "    for pred_tag in predicted_tags_lower:\n",
        "        if pred_tag not in tag_to_sbert:\n",
        "            continue\n",
        "            \n",
        "        pred_embedding = tag_to_sbert[pred_tag]\n",
        "        max_similarity = 0.0\n",
        "        best_match = None\n",
        "        \n",
        "        for gt_tag in ground_truth_tags_lower:\n",
        "            if gt_tag not in tag_to_sbert:\n",
        "                continue\n",
        "                \n",
        "            gt_embedding = tag_to_sbert[gt_tag]\n",
        "            similarity = cosine_similarity([pred_embedding], [gt_embedding])[0][0]\n",
        "            \n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_match = gt_tag\n",
        "        \n",
        "        if max_similarity >= similarity_threshold:\n",
        "            hits.append(pred_tag)\n",
        "            semantic_matches.append({\n",
        "                'predicted': pred_tag,\n",
        "                'matched_gt': best_match,\n",
        "                'similarity': max_similarity\n",
        "            })\n",
        "    \n",
        "    return hits, semantic_matches\n",
        "\n",
        "# Test the semantic similarity function\n",
        "test_pred = ['percussion', 'beat', 'rhythm']\n",
        "test_gt = ['drum', 'drums', 'drumming']\n",
        "test_hits, test_matches = compute_semantic_hits(test_pred, test_gt, tag_to_sbert, SIMILARITY_THRESHOLD)\n",
        "print(f\"Test semantic hits: {test_hits}\")\n",
        "print(f\"Test matches: {test_matches}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate DF-weighted recommendations for all clips with SBERT evaluation\n",
        "print(\"Generating DF-weighted recommendations with SBERT evaluation...\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for pair in tqdm(filtered_pairs):\n",
        "    sound_id = pair['sound_id']\n",
        "    \n",
        "    if sound_id not in audio_embeddings:\n",
        "        continue\n",
        "    \n",
        "    # Get audio embedding\n",
        "    audio_embedding = audio_embeddings[sound_id]\n",
        "    \n",
        "    # Get top 10 DF-weighted recommendations\n",
        "    predicted_tags, prediction_scores, base_scores, df_weights = get_tag_recommendations_df_weighted(\n",
        "        audio_embedding, tag_embeddings, tag_df_scores, top_k=10\n",
        "    )\n",
        "    \n",
        "    # Normalize ground truth tags to lowercase for comparison\n",
        "    ground_truth_tags = [tag.lower() for tag in pair['ground_truth_tags']]\n",
        "    \n",
        "    # Calculate semantic hits using SBERT\n",
        "    semantic_hits, semantic_matches = compute_semantic_hits(\n",
        "        predicted_tags, ground_truth_tags, tag_to_sbert, SIMILARITY_THRESHOLD\n",
        "    )\n",
        "    \n",
        "    # Also compute exact hits for comparison\n",
        "    predicted_tags_lower = [tag.lower() for tag in predicted_tags]\n",
        "    exact_hits = list(set(predicted_tags_lower) & set(ground_truth_tags))\n",
        "    \n",
        "    # Calculate metrics based on semantic hits\n",
        "    precision = len(semantic_hits) / 10.0\n",
        "    recall = len(semantic_hits) / len(ground_truth_tags) if ground_truth_tags else 0.0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    \n",
        "    result = {\n",
        "        'sound_id': sound_id,\n",
        "        'title': pair['title'],\n",
        "        'ground_truth_tags': ground_truth_tags,\n",
        "        'predicted_tags': predicted_tags,\n",
        "        'prediction_scores': prediction_scores,\n",
        "        'base_scores': base_scores,\n",
        "        'df_weights': df_weights,\n",
        "        'semantic_hits': semantic_hits,\n",
        "        'semantic_matches': semantic_matches,\n",
        "        'exact_hits': exact_hits,\n",
        "        'num_semantic_hits': len(semantic_hits),\n",
        "        'num_exact_hits': len(exact_hits),\n",
        "        'precision_at_10': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "    \n",
        "    results.append(result)\n",
        "\n",
        "print(f\"Generated DF-weighted recommendations with SBERT evaluation for {len(results)} clips\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Evaluation and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate overall metrics\n",
        "total_semantic_hits = sum(result['num_semantic_hits'] for result in results)\n",
        "total_exact_hits = sum(result['num_exact_hits'] for result in results)\n",
        "total_predictions = len(results) * 10\n",
        "total_ground_truth = sum(len(result['ground_truth_tags']) for result in results)\n",
        "\n",
        "avg_precision_at_10 = np.mean([result['precision_at_10'] for result in results])\n",
        "avg_recall = np.mean([result['recall'] for result in results])\n",
        "avg_semantic_hits_per_clip = np.mean([result['num_semantic_hits'] for result in results])\n",
        "avg_exact_hits_per_clip = np.mean([result['num_exact_hits'] for result in results])\n",
        "\n",
        "# Calculate F1 scores\n",
        "overall_precision = total_semantic_hits / total_predictions\n",
        "overall_recall = total_semantic_hits / total_ground_truth\n",
        "overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
        "\n",
        "# Calculate average F1 score per clip\n",
        "f1_scores = [result['f1_score'] for result in results]\n",
        "avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "print(f\"=== CLAP DF-Weighted Tag Recommendation Results with SBERT Evaluation ===\")\n",
        "print(f\"Alpha = {ALPHA}, SBERT Threshold = {SIMILARITY_THRESHOLD}\")\n",
        "print(f\"Number of clips evaluated: {len(results)}\")\n",
        "print(f\"Total semantic hits: {total_semantic_hits}\")\n",
        "print(f\"Total exact hits: {total_exact_hits}\")\n",
        "print(f\"Total predictions: {total_predictions}\")\n",
        "print(f\"Total ground truth tags: {total_ground_truth}\")\n",
        "print()\n",
        "print(f\"=== SEMANTIC SIMILARITY METRICS ===\")\n",
        "print(f\"Average Precision@10: {avg_precision_at_10:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
        "print(f\"Average semantic hits per clip: {avg_semantic_hits_per_clip:.2f}\")\n",
        "print(f\"Average exact hits per clip: {avg_exact_hits_per_clip:.2f}\")\n",
        "print()\n",
        "print(f\"Overall Precision: {overall_precision:.4f}\")\n",
        "print(f\"Overall Recall: {overall_recall:.4f}\")\n",
        "print(f\"Overall F1 Score: {overall_f1:.4f}\")\n",
        "print()\n",
        "print(f\"Improvement over exact matching: {(total_semantic_hits / total_exact_hits - 1) * 100:.1f}% more hits\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show some example results with SBERT analysis\n",
        "print(\"=== Example DF-Weighted Results with SBERT Evaluation ===\")\n",
        "\n",
        "# Sort by number of semantic hits (best first)\n",
        "results_sorted = sorted(results, key=lambda x: x['num_semantic_hits'], reverse=True)\n",
        "\n",
        "for i, result in enumerate(results_sorted[:5]):\n",
        "    print(f\"\\n--- Example {i+1} (Sound ID: {result['sound_id']}) ---\")\n",
        "    print(f\"Title: {result['title']}\")\n",
        "    print(f\"Ground Truth Tags: {result['ground_truth_tags']}\")\n",
        "    print(f\"Predicted Tags (with DF weights):\")\n",
        "    for j, (tag, score, base_score, df_weight) in enumerate(zip(\n",
        "        result['predicted_tags'], result['prediction_scores'], \n",
        "        result['base_scores'], result['df_weights']\n",
        "    ), 1):\n",
        "        semantic_hit_marker = \"ðŸ”®\" if tag.lower() in result['semantic_hits'] else \" \"\n",
        "        exact_hit_marker = \"â˜…\" if tag.lower() in result['exact_hits'] else \" \"\n",
        "        print(f\"  {j:2d}.{semantic_hit_marker}{exact_hit_marker} {tag:<20} (W:{score:.4f}, B:{base_score:.4f}, DF:{df_weight:.3f})\")\n",
        "    \n",
        "    print(f\"\\nSemantic Matches (threshold={SIMILARITY_THRESHOLD}):\")\n",
        "    for match in result['semantic_matches']:\n",
        "        print(f\"  '{match['predicted']}' â†” '{match['matched_gt']}' (similarity: {match['similarity']:.3f})\")\n",
        "    \n",
        "    print(f\"\\nExact hits: {result['exact_hits']} ({result['num_exact_hits']} hits)\")\n",
        "    print(f\"Semantic hits: {result['semantic_hits']} ({result['num_semantic_hits']} hits)\")\n",
        "    print(f\"Precision@10: {result['precision_at_10']:.3f}, Recall: {result['recall']:.3f}, F1: {result['f1_score']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of semantic hits vs exact hits\n",
        "semantic_hits_distribution = {}\n",
        "exact_hits_distribution = {}\n",
        "\n",
        "for result in results:\n",
        "    num_semantic = result['num_semantic_hits']\n",
        "    num_exact = result['num_exact_hits']\n",
        "    \n",
        "    semantic_hits_distribution[num_semantic] = semantic_hits_distribution.get(num_semantic, 0) + 1\n",
        "    exact_hits_distribution[num_exact] = exact_hits_distribution.get(num_exact, 0) + 1\n",
        "\n",
        "print(\"\\n=== Distribution Comparison: Semantic vs Exact Hits ===\")\n",
        "print(\"Hits | Semantic Counts | Exact Counts\")\n",
        "print(\"-----|-----------------|-------------\")\n",
        "max_hits = max(max(semantic_hits_distribution.keys()), max(exact_hits_distribution.keys()))\n",
        "for hits in range(max_hits + 1):\n",
        "    semantic_count = semantic_hits_distribution.get(hits, 0)\n",
        "    exact_count = exact_hits_distribution.get(hits, 0)\n",
        "    semantic_pct = semantic_count / len(results) * 100\n",
        "    exact_pct = exact_count / len(results) * 100\n",
        "    print(f\"{hits:4d} | {semantic_count:4d} ({semantic_pct:4.1f}%) | {exact_count:4d} ({exact_pct:4.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert numpy values to Python types for JSON serialization\n",
        "def convert_numpy_types(obj):\n",
        "    \"\"\"Recursively convert numpy types to Python types\"\"\"\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_types(item) for item in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Convert results to be JSON serializable\n",
        "json_safe_results = convert_numpy_types(results)\n",
        "\n",
        "# Save results\n",
        "output_file = f'eval/clap_baseline_df_sbert_alpha{ALPHA}_threshold{SIMILARITY_THRESHOLD}_results.json'\n",
        "os.makedirs('eval', exist_ok=True)\n",
        "\n",
        "# Prepare summary\n",
        "summary = {\n",
        "    'model': f'CLAP DF-Weighted with SBERT (Alpha={ALPHA}, Threshold={SIMILARITY_THRESHOLD})',\n",
        "    'alpha': float(ALPHA),\n",
        "    'sbert_threshold': float(SIMILARITY_THRESHOLD),\n",
        "    'sbert_model': 'all-MiniLM-L6-v2',\n",
        "    'num_clips': len(results),\n",
        "    'total_semantic_hits': int(total_semantic_hits),\n",
        "    'total_exact_hits': int(total_exact_hits),\n",
        "    'total_predictions': int(total_predictions),\n",
        "    'total_ground_truth': int(total_ground_truth),\n",
        "    'avg_precision_at_10': float(avg_precision_at_10),\n",
        "    'avg_recall': float(avg_recall),\n",
        "    'avg_f1_score': float(avg_f1),\n",
        "    'avg_semantic_hits_per_clip': float(avg_semantic_hits_per_clip),\n",
        "    'avg_exact_hits_per_clip': float(avg_exact_hits_per_clip),\n",
        "    'overall_precision': float(overall_precision),\n",
        "    'overall_recall': float(overall_recall),\n",
        "    'overall_f1_score': float(overall_f1),\n",
        "    'semantic_hits_distribution': convert_numpy_types(semantic_hits_distribution),\n",
        "    'exact_hits_distribution': convert_numpy_types(exact_hits_distribution),\n",
        "    'improvement_over_exact': float((total_semantic_hits / total_exact_hits - 1) * 100) if total_exact_hits > 0 else 0.0,\n",
        "    'df_stats': {\n",
        "        'matched_tags': matched_tags,\n",
        "        'unmatched_tags': unmatched_tags,\n",
        "        'min_df_score': float(min(df_scores_values)),\n",
        "        'max_df_score': float(max(df_scores_values)),\n",
        "        'mean_df_score': float(np.mean(df_scores_values)),\n",
        "        'std_df_score': float(np.std(df_scores_values))\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save detailed results\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump({\n",
        "        'summary': summary,\n",
        "        'detailed_results': json_safe_results\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {output_file}\")\n",
        "\n",
        "# Also save summary only\n",
        "summary_file = f'eval/clap_baseline_df_sbert_alpha{ALPHA}_threshold{SIMILARITY_THRESHOLD}_summary.json'\n",
        "with open(summary_file, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"Summary saved to {summary_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "clap2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
