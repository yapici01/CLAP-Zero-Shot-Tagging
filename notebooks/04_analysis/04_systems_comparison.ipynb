{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# RankST vs CLAP Comparison with SBERT Semantic Evaluation\n",
    "\n",
    "This notebook applies SBERT semantic similarity evaluation to compare different tag recommendation systems:\n",
    "1. RankST at k=1, k=2, k=3\n",
    "2. CLAP baseline\n",
    "\n",
    "It loads existing evaluation results and re-evaluates them using semantic similarity with SBERT,\n",
    "providing precision@10, recall, F1 scores, and example outputs for each system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.011666Z",
     "iopub.status.busy": "2025-07-07T14:27:37.011469Z",
     "iopub.status.idle": "2025-07-07T14:27:37.428899Z",
     "shell.execute_reply": "2025-07-07T14:27:37.428617Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Load Ground Truth Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.447388Z",
     "iopub.status.busy": "2025-07-07T14:27:37.447202Z",
     "iopub.status.idle": "2025-07-07T14:27:37.452438Z",
     "shell.execute_reply": "2025-07-07T14:27:37.452168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load input/ground truth pairs\n",
    "with open('data/input_ground_truth_pairs.json', 'r') as f:\n",
    "    input_gt_pairs = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(input_gt_pairs)} sound clips with input/ground truth pairs\")\n",
    "print(f\"Example entry: {input_gt_pairs[0]}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Initialize SBERT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.453822Z",
     "iopub.status.busy": "2025-07-07T14:27:37.453712Z",
     "iopub.status.idle": "2025-07-07T14:27:37.462524Z",
     "shell.execute_reply": "2025-07-07T14:27:37.462124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load SBERT model for semantic similarity\n",
    "print(\"Loading SBERT model...\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"SBERT model loaded successfully!\")\n",
    "\n",
    "# Semantic similarity threshold (adjustable)\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "print(f\"Semantic similarity threshold: {SIMILARITY_THRESHOLD}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Load and Prepare Existing Evaluation Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.464178Z",
     "iopub.status.busy": "2025-07-07T14:27:37.464008Z",
     "iopub.status.idle": "2025-07-07T14:27:37.531588Z",
     "shell.execute_reply": "2025-07-07T14:27:37.531172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load existing evaluation results\n",
    "evaluation_files = {\n",
    "    'clap_baseline': 'eval/clap_baseline_results.json',\n",
    "    'clap_df': 'eval/clap_baseline_df_sbert_space_alpha0.7_threshold0.7_results.json',\n",
    "    'rankst_k1': 'eval/rankst_k1_eval.json',\n",
    "    'rankst_k2': 'eval/rankst_k2_eval.json', \n",
    "    'rankst_k3': 'eval/rankst_k3_eval.json'\n",
    "}\n",
    "\n",
    "existing_results = {}\n",
    "\n",
    "print(\"=== Loading Evaluation Files ===\")\n",
    "for system_name, file_path in evaluation_files.items():\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Found {file_path}\")\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # Inspect file structure\n",
    "            print(f\"  File structure keys: {list(data.keys())}\")\n",
    "            \n",
    "            # Handle different file structures\n",
    "            if 'detailed_results' in data:\n",
    "                # CLAP baseline format\n",
    "                existing_results[system_name] = data['detailed_results']\n",
    "                print(f\"  Using 'detailed_results' key (CLAP format)\")\n",
    "            elif 'evaluation_details' in data:\n",
    "                # RankST format\n",
    "                existing_results[system_name] = data['evaluation_details']\n",
    "                print(f\"  Using 'evaluation_details' key (RankST format)\")\n",
    "            elif 'results' in data:\n",
    "                existing_results[system_name] = data['results']\n",
    "                print(f\"  Using 'results' key\")\n",
    "            else:\n",
    "                existing_results[system_name] = data\n",
    "                print(f\"  Using whole file as results\")\n",
    "                \n",
    "            print(f\"  Loaded {len(existing_results[system_name])} results for {system_name}\")\n",
    "            \n",
    "            # Show sample result structure\n",
    "            if existing_results[system_name]:\n",
    "                sample_result = existing_results[system_name][0]\n",
    "                print(f\"  Sample result keys: {list(sample_result.keys())}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Missing: {file_path}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded results for {len(existing_results)} systems\")\n",
    "if existing_results:\n",
    "    print(f\"Systems: {list(existing_results.keys())}\")\n",
    "else:\n",
    "    print(\"No evaluation files found! Please run the individual evaluation notebooks first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.533459Z",
     "iopub.status.busy": "2025-07-07T14:27:37.533318Z",
     "iopub.status.idle": "2025-07-07T14:27:37.574111Z",
     "shell.execute_reply": "2025-07-07T14:27:37.573706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verify that we loaded the data correctly\n",
    "print(\"=== Data Loading Verification ===\")\n",
    "for system_name, results in existing_results.items():\n",
    "    print(f\"\\n{system_name}:\")\n",
    "    if results:\n",
    "        sample = results[0]\n",
    "        print(f\"  Total results: {len(results)}\")\n",
    "        print(f\"  Sample sound_id: {sample.get('sound_id')}\")\n",
    "        print(f\"  Sample title: {sample.get('title', 'N/A')[:50]}...\")\n",
    "        print(f\"  All sample keys: {list(sample.keys())}\")\n",
    "        \n",
    "        # For debugging, show the actual values in some keys\n",
    "        if 'recommended_tags' in sample:\n",
    "            print(f\"  📋 recommended_tags value: {sample['recommended_tags']}\")\n",
    "            print(f\"  📋 recommended_tags type: {type(sample['recommended_tags'])}\")\n",
    "        if 'ground_truth_tags' in sample:\n",
    "            print(f\"  📋 ground_truth_tags value: {sample['ground_truth_tags']}\")\n",
    "        \n",
    "        # Check for predicted tags field\n",
    "        if 'predicted_tags' in sample:\n",
    "            print(f\"  ✓ Uses 'predicted_tags' (CLAP format)\")\n",
    "            print(f\"    Sample predictions: {sample['predicted_tags'][:3]}\")\n",
    "        elif 'recommended_tags' in sample:\n",
    "            print(f\"  ✓ Uses 'recommended_tags' (RankST format)\")\n",
    "            print(f\"    Sample predictions: {sample['recommended_tags'][:3]}\")\n",
    "        else:\n",
    "            print(f\"  ✗ No predicted/recommended tags found!\")\n",
    "        \n",
    "        # Check for ground truth\n",
    "        if 'ground_truth_tags' in sample:\n",
    "            print(f\"  ✓ Has ground_truth_tags: {sample['ground_truth_tags'][:3]}\")\n",
    "        else:\n",
    "            print(f\"  ⚠ No ground_truth_tags, will use input_gt_pairs\")\n",
    "        \n",
    "        # Check for scores\n",
    "        if 'prediction_scores' in sample:\n",
    "            print(f\"  ✓ Has prediction scores: {sample['prediction_scores'][:3]}\")\n",
    "        else:\n",
    "            print(f\"  - No prediction scores (expected for RankST)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"  ✗ No results loaded\")\n",
    "\n",
    "# Also show raw file structure\n",
    "print(f\"\\n=== RAW FILE STRUCTURE DEBUG ===\")\n",
    "for system_name, file_path in evaluation_files.items():\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"\\n{system_name} ({file_path}):\")\n",
    "        print(f\"  Top-level keys: {list(data.keys())}\")\n",
    "        \n",
    "        if 'evaluation_details' in data and data['evaluation_details']:\n",
    "            sample = data['evaluation_details'][0]\n",
    "            print(f\"  evaluation_details[0] keys: {list(sample.keys())}\")\n",
    "            print(f\"  evaluation_details length: {len(data['evaluation_details'])}\")\n",
    "        if 'detailed_results' in data and data['detailed_results']:\n",
    "            sample = data['detailed_results'][0]\n",
    "            print(f\"  detailed_results[0] keys: {list(sample.keys())}\")\n",
    "            print(f\"  detailed_results length: {len(data['detailed_results'])}\")\n",
    "\n",
    "valid_systems = [s for s in existing_results.values() if s]\n",
    "print(f\"\\nTotal systems successfully loaded: {len(valid_systems)}\")\n",
    "\n",
    "if len(valid_systems) == 0:\n",
    "    print(\"\\n⚠ WARNING: No systems loaded successfully!\")\n",
    "    print(\"Please check that the evaluation files exist and are in the correct format.\")\n",
    "else:\n",
    "    print(\"✓ Data loading appears successful, proceeding with SBERT evaluation...\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Encode Tags with SBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.575916Z",
     "iopub.status.busy": "2025-07-07T14:27:37.575788Z",
     "iopub.status.idle": "2025-07-07T14:27:37.592866Z",
     "shell.execute_reply": "2025-07-07T14:27:37.592536Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect all unique tags from all systems and ground truth\n",
    "print(\"Collecting all unique tags...\")\n",
    "\n",
    "all_tags = set()\n",
    "\n",
    "# Add ground truth tags (normalized)\n",
    "for pair in input_gt_pairs:\n",
    "    normalized_gt_tags = [tag.lower().replace('-', ' ') for tag in pair['ground_truth_tags']]\n",
    "    all_tags.update(normalized_gt_tags)\n",
    "\n",
    "# Add predicted tags from all systems\n",
    "for system_name, results in existing_results.items():\n",
    "    for result in results:\n",
    "        if 'predicted_tags' in result:\n",
    "            # Normalize predicted tags\n",
    "            normalized_pred_tags = [tag.lower().replace('-', ' ') for tag in result['predicted_tags']]\n",
    "            all_tags.update(normalized_pred_tags)\n",
    "        elif 'recommendations' in result:\n",
    "            # Handle different result structures\n",
    "            normalized_pred_tags = [tag.lower().replace('-', ' ') for tag in result['recommendations']]\n",
    "            all_tags.update(normalized_pred_tags)\n",
    "\n",
    "all_unique_tags = list(all_tags)\n",
    "print(f\"Total unique tags to encode: {len(all_unique_tags)}\")\n",
    "\n",
    "# Encode all tags with SBERT\n",
    "print(\"Encoding tags with SBERT...\")\n",
    "tag_sbert_embeddings = sbert_model.encode(all_unique_tags, show_progress_bar=True)\n",
    "print(f\"SBERT embeddings shape: {tag_sbert_embeddings.shape}\")\n",
    "\n",
    "# Create tag to embedding mapping\n",
    "tag_to_sbert = {tag: embedding for tag, embedding in zip(all_unique_tags, tag_sbert_embeddings)}\n",
    "print(\"SBERT encoding completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Define Semantic Similarity Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.594313Z",
     "iopub.status.busy": "2025-07-07T14:27:37.594203Z",
     "iopub.status.idle": "2025-07-07T14:27:37.606064Z",
     "shell.execute_reply": "2025-07-07T14:27:37.605762Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_semantic_hits(predicted_tags, ground_truth_tags, tag_to_sbert, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Compute semantic hits using SBERT embeddings and cosine similarity.\n",
    "    A predicted tag is considered a hit if its semantic similarity\n",
    "    with any ground truth tag exceeds the threshold.\n",
    "    \n",
    "    Note: Both predicted_tags and ground_truth_tags should already be normalized\n",
    "    (lowercase + hyphens replaced with spaces).\n",
    "    \"\"\"\n",
    "    hits = []\n",
    "    semantic_matches = []\n",
    "    \n",
    "    # Normalize predicted tags (lowercase + replace hyphens with spaces)\n",
    "    predicted_tags_normalized = [tag.lower().replace('-', ' ') for tag in predicted_tags]\n",
    "    # ground_truth_tags should already be normalized\n",
    "    \n",
    "    for pred_tag in predicted_tags_normalized:\n",
    "        if pred_tag not in tag_to_sbert:\n",
    "            continue\n",
    "            \n",
    "        pred_embedding = tag_to_sbert[pred_tag]\n",
    "        max_similarity = 0.0\n",
    "        best_match = None\n",
    "        \n",
    "        for gt_tag in ground_truth_tags:\n",
    "            if gt_tag not in tag_to_sbert:\n",
    "                continue\n",
    "                \n",
    "            gt_embedding = tag_to_sbert[gt_tag]\n",
    "            similarity = cosine_similarity([pred_embedding], [gt_embedding])[0][0]\n",
    "            \n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_match = gt_tag\n",
    "        \n",
    "        if max_similarity >= similarity_threshold:\n",
    "            hits.append(pred_tag)\n",
    "            semantic_matches.append({\n",
    "                'predicted': pred_tag,\n",
    "                'matched_gt': best_match,\n",
    "                'similarity': float(max_similarity)\n",
    "            })\n",
    "    \n",
    "    return hits, semantic_matches\n",
    "\n",
    "# Test the semantic similarity function\n",
    "test_pred = ['percussion', 'beat', 'rhythm']\n",
    "test_gt = ['drum', 'drums', 'drumming']\n",
    "# Normalize test ground truth\n",
    "test_gt_normalized = [tag.lower().replace('-', ' ') for tag in test_gt]\n",
    "test_hits, test_matches = compute_semantic_hits(test_pred, test_gt_normalized, tag_to_sbert, SIMILARITY_THRESHOLD)\n",
    "print(f\"Test semantic hits: {test_hits}\")\n",
    "print(f\"Test matches: {test_matches}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Apply SBERT Evaluation to All Systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.607828Z",
     "iopub.status.busy": "2025-07-07T14:27:37.607539Z",
     "iopub.status.idle": "2025-07-07T14:27:37.613433Z",
     "shell.execute_reply": "2025-07-07T14:27:37.613178Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_system_with_sbert(system_name, results_data, tag_to_sbert, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Apply SBERT semantic evaluation to a system's results.\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating {system_name} with SBERT...\")\n",
    "    \n",
    "    evaluated_results = []\n",
    "    \n",
    "    for result in tqdm(results_data, desc=f\"Processing {system_name}\"):\n",
    "        # Handle different result structures for predicted tags\n",
    "        if 'predicted_tags' in result:\n",
    "            # CLAP format\n",
    "            predicted_tags = result['predicted_tags']\n",
    "        elif 'recommended_tags' in result:\n",
    "            # RankST format\n",
    "            predicted_tags = result['recommended_tags']\n",
    "        elif 'recommendations' in result:\n",
    "            predicted_tags = result['recommendations']\n",
    "        else:\n",
    "            print(f\"Warning: No predicted tags found in result for {system_name}\")\n",
    "            print(f\"  Available keys: {list(result.keys())}\")\n",
    "            continue\n",
    "            \n",
    "        # Get ground truth tags\n",
    "        if 'ground_truth_tags' in result:\n",
    "            ground_truth_tags = result['ground_truth_tags']\n",
    "        else:\n",
    "            # Find from input_gt_pairs\n",
    "            sound_id = result.get('sound_id')\n",
    "            gt_pair = next((pair for pair in input_gt_pairs if pair['sound_id'] == sound_id), None)\n",
    "            if gt_pair:\n",
    "                ground_truth_tags = gt_pair['ground_truth_tags']\n",
    "            else:\n",
    "                print(f\"Warning: No ground truth found for sound_id {sound_id}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize ground truth tags\n",
    "        ground_truth_tags_normalized = [tag.lower().replace('-', ' ') for tag in ground_truth_tags]\n",
    "        \n",
    "        # Take top 10 predictions\n",
    "        top_10_predictions = predicted_tags[:10]\n",
    "        \n",
    "        # Calculate semantic hits\n",
    "        semantic_hits, semantic_matches = compute_semantic_hits(\n",
    "            top_10_predictions, ground_truth_tags_normalized, tag_to_sbert, similarity_threshold\n",
    "        )\n",
    "        \n",
    "        # Also compute exact hits for comparison\n",
    "        predicted_tags_normalized = [tag.lower().replace('-', ' ') for tag in top_10_predictions]\n",
    "        exact_hits = list(set(predicted_tags_normalized) & set(ground_truth_tags_normalized))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = len(semantic_hits) / 10.0\n",
    "        recall = len(semantic_hits) / len(ground_truth_tags_normalized) if ground_truth_tags_normalized else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        # Create enhanced result\n",
    "        enhanced_result = {\n",
    "            'sound_id': result.get('sound_id'),\n",
    "            'title': result.get('title', ''),\n",
    "            'ground_truth_tags': ground_truth_tags_normalized,\n",
    "            'predicted_tags': top_10_predictions,\n",
    "            'semantic_hits': semantic_hits,\n",
    "            'semantic_matches': semantic_matches,\n",
    "            'exact_hits': exact_hits,\n",
    "            'num_semantic_hits': len(semantic_hits),\n",
    "            'num_exact_hits': len(exact_hits),\n",
    "            'precision_at_10': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "        \n",
    "        # Add original prediction scores if available (CLAP has these, RankST doesn't)\n",
    "        if 'prediction_scores' in result:\n",
    "            enhanced_result['prediction_scores'] = result['prediction_scores'][:10]\n",
    "        elif 'scores' in result:\n",
    "            enhanced_result['prediction_scores'] = result['scores'][:10]\n",
    "        \n",
    "        evaluated_results.append(enhanced_result)\n",
    "    \n",
    "    return evaluated_results\n",
    "\n",
    "# Apply SBERT evaluation to all systems\n",
    "sbert_evaluated_results = {}\n",
    "\n",
    "print(\"=== Starting SBERT Evaluation ===\")\n",
    "for system_name, results_data in existing_results.items():\n",
    "    if results_data:\n",
    "        print(f\"\\nStarting evaluation for {system_name} with {len(results_data)} results...\")\n",
    "        try:\n",
    "            evaluated = evaluate_system_with_sbert(\n",
    "                system_name, results_data, tag_to_sbert, SIMILARITY_THRESHOLD\n",
    "            )\n",
    "            sbert_evaluated_results[system_name] = evaluated\n",
    "            print(f\"Successfully evaluated {len(evaluated)} results for {system_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {system_name}: {e}\")\n",
    "            sbert_evaluated_results[system_name] = []\n",
    "    else:\n",
    "        print(f\"No data for {system_name}, skipping...\")\n",
    "        sbert_evaluated_results[system_name] = []\n",
    "\n",
    "print(f\"\\nSBERT evaluation completed!\")\n",
    "print(f\"Systems with results: {[k for k, v in sbert_evaluated_results.items() if v]}\")\n",
    "print(f\"Systems without results: {[k for k, v in sbert_evaluated_results.items() if not v]}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Calculate Metrics for All Systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.614679Z",
     "iopub.status.busy": "2025-07-07T14:27:37.614606Z",
     "iopub.status.idle": "2025-07-07T14:27:37.619610Z",
     "shell.execute_reply": "2025-07-07T14:27:37.619339Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_system_metrics(system_results):\n",
    "    \"\"\"\n",
    "    Calculate overall metrics for a system's results.\n",
    "    \"\"\"\n",
    "    if not system_results:\n",
    "        return {}\n",
    "    \n",
    "    total_semantic_hits = sum(result['num_semantic_hits'] for result in system_results)\n",
    "    total_exact_hits = sum(result['num_exact_hits'] for result in system_results)\n",
    "    total_predictions = len(system_results) * 10\n",
    "    total_ground_truth = sum(len(result['ground_truth_tags']) for result in system_results)\n",
    "    \n",
    "    avg_precision_at_10 = np.mean([result['precision_at_10'] for result in system_results])\n",
    "    avg_recall = np.mean([result['recall'] for result in system_results])\n",
    "    avg_f1 = np.mean([result['f1_score'] for result in system_results])\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_precision = total_semantic_hits / total_predictions if total_predictions > 0 else 0\n",
    "    overall_recall = total_semantic_hits / total_ground_truth if total_ground_truth > 0 else 0\n",
    "    overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "    \n",
    "    # Count clips with hits\n",
    "    clips_with_semantic_hits = sum(1 for result in system_results if result['num_semantic_hits'] > 0)\n",
    "    clips_with_exact_hits = sum(1 for result in system_results if result['num_exact_hits'] > 0)\n",
    "    \n",
    "    return {\n",
    "        'num_clips': len(system_results),\n",
    "        'total_semantic_hits': total_semantic_hits,\n",
    "        'total_exact_hits': total_exact_hits,\n",
    "        'total_predictions': total_predictions,\n",
    "        'total_ground_truth': total_ground_truth,\n",
    "        'avg_precision_at_10': avg_precision_at_10,\n",
    "        'avg_recall': avg_recall,\n",
    "        'avg_f1_score': avg_f1,\n",
    "        'overall_precision': overall_precision,\n",
    "        'overall_recall': overall_recall,\n",
    "        'overall_f1_score': overall_f1,\n",
    "        'clips_with_semantic_hits': clips_with_semantic_hits,\n",
    "        'clips_with_exact_hits': clips_with_exact_hits,\n",
    "        'semantic_hit_rate': clips_with_semantic_hits / len(system_results) if system_results else 0,\n",
    "        'exact_hit_rate': clips_with_exact_hits / len(system_results) if system_results else 0\n",
    "    }\n",
    "\n",
    "# Calculate metrics for all systems\n",
    "system_metrics = {}\n",
    "\n",
    "print(\"=== Calculating System Metrics ===\")\n",
    "for system_name, results in sbert_evaluated_results.items():\n",
    "    if results:\n",
    "        print(f\"Calculating metrics for {system_name} with {len(results)} results...\")\n",
    "        try:\n",
    "            metrics = calculate_system_metrics(results)\n",
    "            system_metrics[system_name] = metrics\n",
    "            print(f\"Success! {system_name} metrics: P@10={metrics['avg_precision_at_10']:.3f}, F1={metrics['avg_f1_score']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating metrics for {system_name}: {e}\")\n",
    "            system_metrics[system_name] = {}\n",
    "    else:\n",
    "        print(f\"No results for {system_name}, creating empty metrics...\")\n",
    "        system_metrics[system_name] = {}\n",
    "\n",
    "print(f\"\\nMetrics calculation completed!\")\n",
    "print(f\"Systems with metrics: {[k for k, v in system_metrics.items() if v]}\")\n",
    "print(f\"Systems without metrics: {[k for k, v in system_metrics.items() if not v]}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Display Results Comparison\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9. Visualization Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Prepare data for plotting\n",
    "systems_data = []\n",
    "for system_name in systems_order:\n",
    "    if system_name in system_metrics and system_metrics[system_name]:\n",
    "        metrics = system_metrics[system_name]\n",
    "        display_name = system_display_names.get(system_name, system_name)\n",
    "        \n",
    "        # Calculate exact matching metrics\n",
    "        exact_precision = metrics['total_exact_hits'] / (metrics['num_clips'] * 10) if metrics['num_clips'] > 0 else 0\n",
    "        exact_recall = metrics['total_exact_hits'] / metrics['total_ground_truth'] if metrics['total_ground_truth'] > 0 else 0\n",
    "        exact_f1 = 2 * (exact_precision * exact_recall) / (exact_precision + exact_recall) if (exact_precision + exact_recall) > 0 else 0\n",
    "        \n",
    "        # Calculate improvements\n",
    "        hits_improvement = ((metrics['total_semantic_hits'] / metrics['total_exact_hits']) - 1) * 100 if metrics['total_exact_hits'] > 0 else 0\n",
    "        clips_improvement = ((metrics['clips_with_semantic_hits'] / metrics['clips_with_exact_hits']) - 1) * 100 if metrics['clips_with_exact_hits'] > 0 else 0\n",
    "        f1_improvement = ((metrics['overall_f1_score'] / exact_f1) - 1) * 100 if exact_f1 > 0 else 0\n",
    "        \n",
    "        systems_data.append({\n",
    "            'system': display_name,\n",
    "            'system_type': 'Zero-Shot CLAP' if 'CLAP' in display_name else 'Traditional RankST',\n",
    "            'exact_precision': exact_precision,\n",
    "            'exact_recall': exact_recall,\n",
    "            'exact_f1': exact_f1,\n",
    "            'semantic_precision': metrics['overall_precision'],\n",
    "            'semantic_recall': metrics['overall_recall'], \n",
    "            'semantic_f1': metrics['overall_f1_score'],\n",
    "            'exact_hit_rate': metrics['exact_hit_rate'],\n",
    "            'semantic_hit_rate': metrics['semantic_hit_rate'],\n",
    "            'hits_improvement': hits_improvement,\n",
    "            'clips_improvement': clips_improvement,\n",
    "            'f1_improvement': f1_improvement,\n",
    "            'total_exact_hits': metrics['total_exact_hits'],\n",
    "            'total_semantic_hits': metrics['total_semantic_hits'],\n",
    "            'clips_exact': metrics['clips_with_exact_hits'],\n",
    "            'clips_semantic': metrics['clips_with_semantic_hits']\n",
    "        })\n",
    "\n",
    "# Create the comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Audio Tag Recommendation: Zero-Shot CLAP vs Traditional RankST\\nSemantic Evaluation with SBERT', \n",
    "             fontsize=16, fontweight='bold', y=0.95)\n",
    "\n",
    "# Extract data for plotting\n",
    "systems = [d['system'] for d in systems_data]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']\n",
    "\n",
    "# 1. Overall Performance Comparison (Semantic)\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(systems))\n",
    "width = 0.25\n",
    "\n",
    "f1_scores = [d['semantic_f1'] for d in systems_data]\n",
    "precision_scores = [d['semantic_precision'] for d in systems_data]\n",
    "recall_scores = [d['semantic_recall'] for d in systems_data]\n",
    "\n",
    "bars1 = ax1.bar(x_pos - width, f1_scores, width, label='F1 Score', alpha=0.8)\n",
    "bars2 = ax1.bar(x_pos, precision_scores, width, label='Precision@10', alpha=0.8)\n",
    "bars3 = ax1.bar(x_pos + width, recall_scores, width, label='Recall', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('System')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Overall Performance (Semantic Matching)', fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(systems, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 2. Exact vs Semantic F1 Comparison\n",
    "ax2 = axes[0, 1]\n",
    "x_pos = np.arange(len(systems))\n",
    "width = 0.35\n",
    "\n",
    "exact_f1 = [d['exact_f1'] for d in systems_data]\n",
    "semantic_f1 = [d['semantic_f1'] for d in systems_data]\n",
    "\n",
    "bars1 = ax2.bar(x_pos - width/2, exact_f1, width, label='Exact Matching', alpha=0.7, color='lightcoral')\n",
    "bars2 = ax2.bar(x_pos + width/2, semantic_f1, width, label='Semantic Matching', alpha=0.7, color='lightblue')\n",
    "\n",
    "ax2.set_xlabel('System')\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_title('Exact vs Semantic Matching Performance', fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(systems, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 3. Improvement Percentages\n",
    "ax3 = axes[0, 2]\n",
    "improvements_data = {\n",
    "    'F1 Improvement': [d['f1_improvement'] for d in systems_data],\n",
    "    'Total Hits': [d['hits_improvement'] for d in systems_data],\n",
    "    'Clips+ Improvement': [d['clips_improvement'] for d in systems_data]\n",
    "}\n",
    "\n",
    "x_pos = np.arange(len(systems))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax3.bar(x_pos - width, improvements_data['F1 Improvement'], width, \n",
    "               label='F1+%', alpha=0.8, color='gold')\n",
    "bars2 = ax3.bar(x_pos, improvements_data['Total Hits'], width, \n",
    "               label='% More Hits', alpha=0.8, color='lightgreen')\n",
    "bars3 = ax3.bar(x_pos + width, improvements_data['Clips+ Improvement'], width, \n",
    "               label='Clips+%', alpha=0.8, color='plum')\n",
    "\n",
    "ax3.set_xlabel('System')\n",
    "ax3.set_ylabel('Improvement (%)')\n",
    "ax3.set_title('Semantic Matching Improvements', fontweight='bold')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(systems, rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{height:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 4. Hit Rate Analysis\n",
    "ax4 = axes[1, 0]\n",
    "exact_rates = [d['exact_hit_rate'] * 100 for d in systems_data]\n",
    "semantic_rates = [d['semantic_hit_rate'] * 100 for d in systems_data]\n",
    "\n",
    "x_pos = np.arange(len(systems))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x_pos - width/2, exact_rates, width, label='Exact Hit Rate', alpha=0.7, color='lightcoral')\n",
    "bars2 = ax4.bar(x_pos + width/2, semantic_rates, width, label='Semantic Hit Rate', alpha=0.7, color='lightblue')\n",
    "\n",
    "ax4.set_xlabel('System')\n",
    "ax4.set_ylabel('Hit Rate (%)')\n",
    "ax4.set_title('Clips Getting ≥1 Hit (%)', fontweight='bold')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(systems, rotation=45, ha='right')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 5. RankST Scaling Analysis\n",
    "ax5 = axes[1, 1]\n",
    "rankst_systems = [d for d in systems_data if 'RankST' in d['system']]\n",
    "k_values = [1, 2, 3]\n",
    "rankst_f1 = [d['semantic_f1'] for d in rankst_systems]\n",
    "rankst_precision = [d['semantic_precision'] for d in rankst_systems]\n",
    "rankst_recall = [d['semantic_recall'] for d in rankst_systems]\n",
    "\n",
    "ax5.plot(k_values, rankst_f1, 'o-', linewidth=2, markersize=8, label='F1 Score', color='blue')\n",
    "ax5.plot(k_values, rankst_precision, 's-', linewidth=2, markersize=8, label='Precision@10', color='green')\n",
    "ax5.plot(k_values, rankst_recall, '^-', linewidth=2, markersize=8, label='Recall', color='red')\n",
    "\n",
    "ax5.set_xlabel('Input Tags (k)')\n",
    "ax5.set_ylabel('Score')\n",
    "ax5.set_title('RankST Performance Scaling', fontweight='bold')\n",
    "ax5.set_xticks(k_values)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (f1, prec, rec) in enumerate(zip(rankst_f1, rankst_precision, rankst_recall)):\n",
    "    ax5.text(k_values[i], f1 + 0.01, f'{f1:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    ax5.text(k_values[i], prec + 0.01, f'{prec:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    ax5.text(k_values[i], rec + 0.01, f'{rec:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 6. Zero-Shot vs Traditional Comparison\n",
    "ax6 = axes[1, 2]\n",
    "clap_systems = [d for d in systems_data if 'CLAP' in d['system']]\n",
    "rankst_best = max(rankst_systems, key=lambda x: x['semantic_f1'])\n",
    "\n",
    "comparison_data = {\n",
    "    'System Type': ['CLAP\\nBaseline', 'CLAP\\nwith DF', 'RankST\\n(Best k=3)'],\n",
    "    'F1 Score': [clap_systems[0]['semantic_f1'], clap_systems[1]['semantic_f1'], rankst_best['semantic_f1']],\n",
    "    'Training': ['Zero-Shot', 'Zero-Shot', 'Requires Training'],\n",
    "    'Colors': ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "}\n",
    "\n",
    "bars = ax6.bar(comparison_data['System Type'], comparison_data['F1 Score'], \n",
    "               color=comparison_data['Colors'], alpha=0.8)\n",
    "\n",
    "ax6.set_ylabel('F1 Score (Semantic)')\n",
    "ax6.set_title('Zero-Shot vs Traditional Approaches', fontweight='bold')\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels and training info\n",
    "for i, (bar, training) in enumerate(zip(bars, comparison_data['Training'])):\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "            f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height/2,\n",
    "            training, ha='center', va='center', fontsize=8, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.91)\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Visualization Summary:\")\n",
    "print(\"─\" * 50)\n",
    "print(\"1. Overall Performance: RankST k=3 leads, but requires training data\")\n",
    "print(\"2. Semantic vs Exact: All systems benefit significantly from semantic matching\")\n",
    "print(\"3. Improvements: CLAP systems show larger relative improvements\")\n",
    "print(\"4. Hit Rates: RankST reaches more clips, but CLAP improves more with semantics\")\n",
    "print(\"5. RankST Scaling: Performance increases consistently with more input tags\")\n",
    "print(\"6. Zero-Shot Advantage: CLAP works immediately without training data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two separate comparison graphs for clearer visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Position setup for grouped bars\n",
    "x_pos = np.arange(len(systems_names))\n",
    "width = 0.35\n",
    "\n",
    "# GRAPH 1: F1 Score Comparison\n",
    "f1_exact_bars = ax1.bar(x_pos - width/2, exact_f1_scores, width, \n",
    "                       label='Exact Matching', alpha=0.8, color='lightcoral', \n",
    "                       edgecolor='darkred', linewidth=1.5)\n",
    "f1_semantic_bars = ax1.bar(x_pos + width/2, semantic_f1_scores, width, \n",
    "                          label='Semantic Matching', alpha=0.8, color='lightblue',\n",
    "                          edgecolor='darkblue', linewidth=1.5)\n",
    "\n",
    "ax1.set_xlabel('System', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('F1 Score: Exact vs Semantic Matching', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(systems_names, rotation=45, ha='right')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels and improvement percentages for F1\n",
    "for i, (f1_exact, f1_semantic) in enumerate(zip(exact_f1_scores, semantic_f1_scores)):\n",
    "    # F1 exact value\n",
    "    ax1.text(x_pos[i] - width/2, f1_exact + 0.003,\n",
    "            f'{f1_exact:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # F1 semantic value\n",
    "    ax1.text(x_pos[i] + width/2, f1_semantic + 0.003,\n",
    "            f'{f1_semantic:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # F1 improvement percentage\n",
    "    f1_improvement = ((f1_semantic / f1_exact) - 1) * 100 if f1_exact > 0 else 0\n",
    "    max_f1_height = max(f1_exact, f1_semantic)\n",
    "    ax1.text(x_pos[i], max_f1_height + 0.015, f'+{f1_improvement:.0f}%', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold', \n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='gold', alpha=0.8))\n",
    "\n",
    "# GRAPH 2: Total Hits Comparison\n",
    "hits_exact_bars = ax2.bar(x_pos - width/2, exact_total_hits, width, \n",
    "                         label='Exact Matching', alpha=0.8, color='lightcoral',\n",
    "                         edgecolor='darkred', linewidth=1.5)\n",
    "hits_semantic_bars = ax2.bar(x_pos + width/2, semantic_total_hits, width, \n",
    "                            label='Semantic Matching', alpha=0.8, color='lightblue',\n",
    "                            edgecolor='darkblue', linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel('System', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Total Hits', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Total Hits: Exact vs Semantic Matching', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(systems_names, rotation=45, ha='right')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels and improvement percentages for Total Hits\n",
    "for i, (hits_exact, hits_semantic) in enumerate(zip(exact_total_hits, semantic_total_hits)):\n",
    "    # Hits exact value\n",
    "    ax2.text(x_pos[i] - width/2, hits_exact + 50,\n",
    "            f'{int(hits_exact)}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Hits semantic value\n",
    "    ax2.text(x_pos[i] + width/2, hits_semantic + 50,\n",
    "            f'{int(hits_semantic)}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Hits improvement percentage\n",
    "    hits_improvement = ((hits_semantic / hits_exact) - 1) * 100 if hits_exact > 0 else 0\n",
    "    max_hits_height = max(hits_exact, hits_semantic)\n",
    "    ax2.text(x_pos[i], max_hits_height + 150, f'+{hits_improvement:.0f}%', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold', \n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='gold', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison summary\n",
    "print(\"\\n📊 F1 Score & Total Hits Comparison Summary:\")\n",
    "print(\"=\" * 85)\n",
    "print(f\"{'System':<15} {'F1 Exact':<10} {'F1 Semantic':<12} {'F1 Δ%':<8} {'Hits Exact':<12} {'Hits Semantic':<14} {'Hits Δ%':<8}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for i, system_name in enumerate(systems_names):\n",
    "    f1_improvement = ((semantic_f1_scores[i] / exact_f1_scores[i]) - 1) * 100 if exact_f1_scores[i] > 0 else 0\n",
    "    hits_improvement = ((semantic_total_hits[i] / exact_total_hits[i]) - 1) * 100 if exact_total_hits[i] > 0 else 0\n",
    "    \n",
    "    print(f\"{system_name:<15} {exact_f1_scores[i]:<10.3f} {semantic_f1_scores[i]:<12.3f} {f1_improvement:<8.0f}% \"\n",
    "          f\"{exact_total_hits[i]:<12d} {semantic_total_hits[i]:<14d} {hits_improvement:<8.0f}%\")\n",
    "\n",
    "print(\"\\n💡 Key Insights from Separate Graphs:\")\n",
    "print(\"─\" * 50)\n",
    "print(\"• F1 Quality: RankST k=3 best absolute (0.284), CLAP Baseline best improvement\")\n",
    "print(\"• Total Volume: RankST k=3 achieves highest hits vs CLAP systems\")  \n",
    "print(\"• Zero-Shot Advantage: CLAP systems work immediately without training data\")\n",
    "print(\"• Semantic Benefit: All systems improve significantly with semantic matching\")\n",
    "print(\"• Research Value: Zero-shot shows higher improvement potential\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. System Comparison Graphs\n",
    "\n",
    "# First, let's debug the percentage calculations by printing the actual values\n",
    "print(\"🔍 DEBUGGING PERCENTAGE CALCULATIONS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data for all systems\n",
    "systems_names = [system_display_names[s] for s in systems_order if s in system_metrics and system_metrics[s]]\n",
    "exact_f1_scores = []\n",
    "semantic_f1_scores = []\n",
    "exact_total_hits = []\n",
    "semantic_total_hits = []\n",
    "\n",
    "for system_name in systems_order:\n",
    "    if system_name in system_metrics and system_metrics[system_name]:\n",
    "        metrics = system_metrics[system_name]\n",
    "        \n",
    "        # Calculate exact F1\n",
    "        exact_precision = metrics['total_exact_hits'] / (metrics['num_clips'] * 10) if metrics['num_clips'] > 0 else 0\n",
    "        exact_recall = metrics['total_exact_hits'] / metrics['total_ground_truth'] if metrics['total_ground_truth'] > 0 else 0\n",
    "        exact_f1 = 2 * (exact_precision * exact_recall) / (exact_precision + exact_recall) if (exact_precision + exact_recall) > 0 else 0\n",
    "        \n",
    "        exact_f1_scores.append(exact_f1)\n",
    "        semantic_f1_scores.append(metrics['overall_f1_score'])\n",
    "        exact_total_hits.append(metrics['total_exact_hits'])\n",
    "        semantic_total_hits.append(metrics['total_semantic_hits'])\n",
    "        \n",
    "        # Debug print for each system\n",
    "        system_display = system_display_names.get(system_name, system_name)\n",
    "        f1_improvement = ((metrics['overall_f1_score'] / exact_f1) - 1) * 100 if exact_f1 > 0 else 0\n",
    "        hits_improvement = ((metrics['total_semantic_hits'] / metrics['total_exact_hits']) - 1) * 100 if metrics['total_exact_hits'] > 0 else 0\n",
    "        \n",
    "        print(f\"{system_display}:\")\n",
    "        print(f\"  F1: {exact_f1:.6f} → {metrics['overall_f1_score']:.6f} = +{f1_improvement:.1f}%\")\n",
    "        print(f\"  Hits: {metrics['total_exact_hits']} → {metrics['total_semantic_hits']} = +{hits_improvement:.1f}%\")\n",
    "        print()\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. System Comparison Graphs\n",
    "\n",
    "# Two separate comparison graphs for clearer visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Prepare data for all systems\n",
    "systems_names = [system_display_names[s] for s in systems_order if s in system_metrics and system_metrics[s]]\n",
    "exact_f1_scores = []\n",
    "semantic_f1_scores = []\n",
    "exact_total_hits = []\n",
    "semantic_total_hits = []\n",
    "\n",
    "for system_name in systems_order:\n",
    "    if system_name in system_metrics and system_metrics[system_name]:\n",
    "        metrics = system_metrics[system_name]\n",
    "        \n",
    "        # Calculate exact F1\n",
    "        exact_precision = metrics['total_exact_hits'] / (metrics['num_clips'] * 10) if metrics['num_clips'] > 0 else 0\n",
    "        exact_recall = metrics['total_exact_hits'] / metrics['total_ground_truth'] if metrics['total_ground_truth'] > 0 else 0\n",
    "        exact_f1 = 2 * (exact_precision * exact_recall) / (exact_precision + exact_recall) if (exact_precision + exact_recall) > 0 else 0\n",
    "        \n",
    "        exact_f1_scores.append(exact_f1)\n",
    "        semantic_f1_scores.append(metrics['overall_f1_score'])\n",
    "        exact_total_hits.append(metrics['total_exact_hits'])\n",
    "        semantic_total_hits.append(metrics['total_semantic_hits'])\n",
    "\n",
    "# Position setup for grouped bars\n",
    "x_pos = np.arange(len(systems_names))\n",
    "width = 0.35\n",
    "\n",
    "# GRAPH 1: F1 Score Comparison\n",
    "f1_exact_bars = ax1.bar(x_pos - width/2, exact_f1_scores, width, \n",
    "                       label='Exact Matching', alpha=0.8, color='lightcoral', \n",
    "                       edgecolor='darkred', linewidth=1.5)\n",
    "f1_semantic_bars = ax1.bar(x_pos + width/2, semantic_f1_scores, width, \n",
    "                          label='Semantic Matching', alpha=0.8, color='lightblue',\n",
    "                          edgecolor='darkblue', linewidth=1.5)\n",
    "\n",
    "ax1.set_xlabel('System', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('F1 Score: Exact vs Semantic Matching', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(systems_names, rotation=45, ha='right')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels and improvement percentages for F1\n",
    "for i, (bar_exact, bar_semantic) in enumerate(zip(f1_exact_bars, f1_semantic_bars)):\n",
    "    # F1 exact\n",
    "    f1_height_exact = bar_exact.get_height()\n",
    "    ax1.text(bar_exact.get_x() + bar_exact.get_width()/2., f1_height_exact + 0.003,\n",
    "            f'{f1_height_exact:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # F1 semantic\n",
    "    f1_height_semantic = bar_semantic.get_height()\n",
    "    ax1.text(bar_semantic.get_x() + bar_semantic.get_width()/2., f1_height_semantic + 0.003,\n",
    "            f'{f1_height_semantic:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Improvement percentage (using original F1 data arrays)\n",
    "    f1_improvement = ((semantic_f1_scores[i] / exact_f1_scores[i]) - 1) * 100 if exact_f1_scores[i] > 0 else 0\n",
    "    max_f1_height = max(f1_height_exact, f1_height_semantic)\n",
    "    ax1.text(x_pos[i], max_f1_height + 0.015, f'+{f1_improvement:.0f}%', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold', \n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='gold', alpha=0.8))\n",
    "\n",
    "# GRAPH 2: Total Hits Comparison\n",
    "hits_exact_bars = ax2.bar(x_pos - width/2, exact_total_hits, width, \n",
    "                         label='Exact Matching', alpha=0.8, color='lightcoral',\n",
    "                         edgecolor='darkred', linewidth=1.5)\n",
    "hits_semantic_bars = ax2.bar(x_pos + width/2, semantic_total_hits, width, \n",
    "                            label='Semantic Matching', alpha=0.8, color='lightblue',\n",
    "                            edgecolor='darkblue', linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel('System', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Total Hits', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Total Hits: Exact vs Semantic Matching', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(systems_names, rotation=45, ha='right')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels and improvement percentages for Total Hits\n",
    "for i, (bar_exact, bar_semantic) in enumerate(zip(hits_exact_bars, hits_semantic_bars)):\n",
    "    # Hits exact\n",
    "    hits_height_exact = bar_exact.get_height()\n",
    "    ax2.text(bar_exact.get_x() + bar_exact.get_width()/2., hits_height_exact + 50,\n",
    "            f'{int(hits_height_exact)}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Hits semantic\n",
    "    hits_height_semantic = bar_semantic.get_height()\n",
    "    ax2.text(bar_semantic.get_x() + bar_semantic.get_width()/2., hits_height_semantic + 50,\n",
    "            f'{int(hits_height_semantic)}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Improvement percentage (using original hits data arrays)\n",
    "    hits_improvement = ((semantic_total_hits[i] / exact_total_hits[i]) - 1) * 100 if exact_total_hits[i] > 0 else 0\n",
    "    max_hits_height = max(hits_height_exact, hits_height_semantic)\n",
    "    ax2.text(x_pos[i], max_hits_height + 150, f'+{hits_improvement:.0f}%', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold', \n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='gold', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison summary\n",
    "print(\"\\n📊 F1 Score & Total Hits Comparison Summary:\")\n",
    "print(\"=\" * 85)\n",
    "print(f\"{'System':<15} {'F1 Exact':<10} {'F1 Semantic':<12} {'F1 Δ%':<8} {'Hits Exact':<12} {'Hits Semantic':<14} {'Hits Δ%':<8}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for i, system_name in enumerate(systems_names):\n",
    "    f1_improvement = ((semantic_f1_scores[i] / exact_f1_scores[i]) - 1) * 100 if exact_f1_scores[i] > 0 else 0\n",
    "    hits_improvement = ((semantic_total_hits[i] / exact_total_hits[i]) - 1) * 100 if exact_total_hits[i] > 0 else 0\n",
    "    \n",
    "    print(f\"{system_name:<15} {exact_f1_scores[i]:<10.3f} {semantic_f1_scores[i]:<12.3f} {f1_improvement:<8.0f}% \"\n",
    "          f\"{exact_total_hits[i]:<12d} {semantic_total_hits[i]:<14d} {hits_improvement:<8.0f}%\")\n",
    "\n",
    "print(\"\\n💡 Key Insights from Separate Graphs:\")\n",
    "print(\"─\" * 50)\n",
    "print(\"• F1 Quality: RankST k=3 best absolute (0.284), CLAP Baseline best improvement (+131%)\")\n",
    "print(\"• Total Volume: RankST k=3 achieves 3,325 hits vs CLAP DF's 746 hits\")  \n",
    "print(\"• Zero-Shot Advantage: CLAP systems work immediately without training data\")\n",
    "print(\"• Semantic Benefit: All systems improve significantly with semantic matching\")\n",
    "print(\"• Research Value: Zero-shot shows higher improvement potential\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.620952Z",
     "iopub.status.busy": "2025-07-07T14:27:37.620849Z",
     "iopub.status.idle": "2025-07-07T14:27:37.646146Z",
     "shell.execute_reply": "2025-07-07T14:27:37.645860Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display comprehensive comparison table\n",
    "print(\"=\" * 160)\n",
    "print(\"🎯 EXACT vs SEMANTIC MATCHING COMPARISON\")\n",
    "print(\"=\" * 160)\n",
    "print(f\"SBERT Model: all-MiniLM-L6-v2 | Similarity Threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(\"Legend: Hits=Total hits, Clips+=Clips with ≥1 hit, F1+%=F1 improvement %, % More=% more total hits, Clips+%=% more clips with hits, HitsPC=% improvement in avg hits per successful clip\")\n",
    "print(\"=\" * 160)\n",
    "\n",
    "# Create comparison table\n",
    "systems_order = ['clap_baseline', 'clap_df', 'rankst_k1', 'rankst_k2', 'rankst_k3']\n",
    "system_display_names = {\n",
    "    'clap_baseline': 'CLAP Baseline',\n",
    "    'clap_df': 'CLAP with DF',\n",
    "    'rankst_k1': 'RankST k=1',\n",
    "    'rankst_k2': 'RankST k=2', \n",
    "    'rankst_k3': 'RankST k=3'\n",
    "}\n",
    "\n",
    "# Check if we have any valid metrics\n",
    "valid_systems = [s for s in systems_order if s in system_metrics and system_metrics[s]]\n",
    "\n",
    "if not valid_systems:\n",
    "    print(\"❌ No valid system metrics found. Please check that the evaluation files exist and are properly formatted.\")\n",
    "    print(f\"Expected files: {list(evaluation_files.values())}\")\n",
    "else:\n",
    "    # Header for comparison table  \n",
    "    print(f\"{'System':<15} {'Clips':<6} │ {'EXACT MATCHING':<50} │ {'SEMANTIC MATCHING':<50} │ {'IMPROVEMENT':<42}\")\n",
    "    print(f\"{'':15} {'':6} │ {'Hits':<6} {'Clips+':<7} {'P@10':<8} {'Recall':<8} {'F1':<8} │ {'Hits':<6} {'Clips+':<7} {'P@10':<8} {'Recall':<8} {'F1':<8} │ {'F1+%':<6} {'% More':<7} {'Clips+%':<8} {'HitsPC':<8}\")\n",
    "    print(\"─\" * 160)\n",
    "\n",
    "    for system_name in systems_order:\n",
    "        if system_name in system_metrics and system_metrics[system_name]:\n",
    "            metrics = system_metrics[system_name]\n",
    "            display_name = system_display_names.get(system_name, system_name)\n",
    "            \n",
    "            # Calculate exact matching metrics for comparison\n",
    "            exact_precision = metrics['total_exact_hits'] / (metrics['num_clips'] * 10) if metrics['num_clips'] > 0 else 0\n",
    "            exact_recall = metrics['total_exact_hits'] / metrics['total_ground_truth'] if metrics['total_ground_truth'] > 0 else 0\n",
    "            exact_f1 = 2 * (exact_precision * exact_recall) / (exact_precision + exact_recall) if (exact_precision + exact_recall) > 0 else 0\n",
    "            \n",
    "            # F1 improvement calculation\n",
    "            f1_improvement = ((metrics['overall_f1_score'] / exact_f1) - 1) * 100 if exact_f1 > 0 else float('inf') if metrics['overall_f1_score'] > 0 else 0\n",
    "            f1_improvement_str = f\"+{f1_improvement:.0f}%\" if f1_improvement != float('inf') else \"∞\"\n",
    "            \n",
    "            # Improvement calculations\n",
    "            hits_improvement = ((metrics['total_semantic_hits'] / metrics['total_exact_hits']) - 1) * 100 if metrics['total_exact_hits'] > 0 else float('inf') if metrics['total_semantic_hits'] > 0 else 0\n",
    "            hits_improvement_str = f\"+{hits_improvement:.0f}%\" if hits_improvement != float('inf') else \"∞\"\n",
    "            \n",
    "            # Clips improvement calculation - for clips that get hits\n",
    "            clips_improvement = ((metrics['clips_with_semantic_hits'] / metrics['clips_with_exact_hits']) - 1) * 100 if metrics['clips_with_exact_hits'] > 0 else float('inf') if metrics['clips_with_semantic_hits'] > 0 else 0\n",
    "            clips_improvement_str = f\"+{clips_improvement:.0f}%\" if clips_improvement != float('inf') else \"∞\"\n",
    "            \n",
    "            # Hits per clip improvement - for clips that actually have hits\n",
    "            # This shows: \"For clips that get hits, how much better is the average hits per clip?\"\n",
    "            exact_avg_hits_per_successful_clip = metrics['total_exact_hits'] / metrics['clips_with_exact_hits'] if metrics['clips_with_exact_hits'] > 0 else 0\n",
    "            semantic_avg_hits_per_successful_clip = metrics['total_semantic_hits'] / metrics['clips_with_semantic_hits'] if metrics['clips_with_semantic_hits'] > 0 else 0\n",
    "            hits_pc_improvement = ((semantic_avg_hits_per_successful_clip / exact_avg_hits_per_successful_clip) - 1) * 100 if exact_avg_hits_per_successful_clip > 0 else float('inf') if semantic_avg_hits_per_successful_clip > 0 else 0\n",
    "            hits_pc_str = f\"+{hits_pc_improvement:.0f}%\" if hits_pc_improvement != float('inf') else \"∞\"\n",
    "            \n",
    "            print(f\"{display_name:<15} {metrics['num_clips']:<6} │ \"\n",
    "                  f\"{metrics['total_exact_hits']:<6} {metrics['clips_with_exact_hits']:<7} {exact_precision:<8.3f} {exact_recall:<8.3f} {exact_f1:<8.3f} │ \"\n",
    "                  f\"{metrics['total_semantic_hits']:<6} {metrics['clips_with_semantic_hits']:<7} {metrics['overall_precision']:<8.3f} {metrics['overall_recall']:<8.3f} {metrics['overall_f1_score']:<8.3f} │ \"\n",
    "                  f\"{f1_improvement_str:<6} {hits_improvement_str:<7} {clips_improvement_str:<8} {hits_pc_str:<8}\")\n",
    "        else:\n",
    "            display_name = system_display_names.get(system_name, system_name)\n",
    "            print(f\"{display_name:<15} {'N/A':<6} │ {'N/A':<50} │ {'N/A':<50} │ {'N/A':<42}\")\n",
    "\n",
    "    print(\"=\" * 160)\n",
    "\n",
    "    # Performance ranking\n",
    "    print(\"\\n🏆 PERFORMANCE RANKING (by Semantic F1 Score)\")\n",
    "    print(\"─\" * 60)\n",
    "    \n",
    "    # Sort systems by semantic F1 score\n",
    "    ranked_systems = [(name, metrics) for name, metrics in system_metrics.items() if metrics]\n",
    "    ranked_systems.sort(key=lambda x: x[1]['overall_f1_score'], reverse=True)\n",
    "    \n",
    "    for i, (system_name, metrics) in enumerate(ranked_systems, 1):\n",
    "        display_name = system_display_names.get(system_name, system_name)\n",
    "        medal = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\" if i == 3 else f\"{i}.\"\n",
    "        print(f\"{medal:<3} {display_name:<15} F1: {metrics['overall_f1_score']:.4f} | P@10: {metrics['overall_precision']:.4f} | Recall: {metrics['overall_recall']:.4f}\")\n",
    "    \n",
    "    print(\"=\" * 160)\n",
    "\n",
    "    # Hit rate comparison\n",
    "    print(\"\\n📊 HIT RATE COMPARISON\")\n",
    "    print(\"─\" * 80)\n",
    "    print(f\"{'System':<15} {'Exact Hit Rate':<15} {'Semantic Hit Rate':<18} {'Improvement':<12}\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    for system_name in systems_order:\n",
    "        if system_name in system_metrics and system_metrics[system_name]:\n",
    "            metrics = system_metrics[system_name]\n",
    "            display_name = system_display_names.get(system_name, system_name)\n",
    "            \n",
    "            improvement = metrics['semantic_hit_rate'] - metrics['exact_hit_rate']\n",
    "            improvement_str = f\"+{improvement:.1%}\"\n",
    "            \n",
    "            print(f\"{display_name:<15} {metrics['exact_hit_rate']:<15.1%} {metrics['semantic_hit_rate']:<18.1%} {improvement_str:<12}\")\n",
    "\n",
    "    print(\"=\" * 160)\n",
    "\n",
    "    # Summary insights\n",
    "    print(\"\\n💡 KEY INSIGHTS\")\n",
    "    print(\"─\" * 50)\n",
    "    best_system = max(ranked_systems, key=lambda x: x[1]['overall_f1_score'])\n",
    "    best_improvement_system = max(ranked_systems, key=lambda x: (x[1]['total_semantic_hits'] / x[1]['total_exact_hits']) if x[1]['total_exact_hits'] > 0 else 0)\n",
    "    \n",
    "    print(f\"• Best Overall Performance: {system_display_names[best_system[0]]} (F1: {best_system[1]['overall_f1_score']:.4f})\")\n",
    "    \n",
    "    if best_improvement_system[1]['total_exact_hits'] > 0:\n",
    "        improvement_pct = ((best_improvement_system[1]['total_semantic_hits'] / best_improvement_system[1]['total_exact_hits']) - 1) * 100\n",
    "        print(f\"• Largest Improvement: {system_display_names[best_improvement_system[0]]} (+{improvement_pct:.0f}% more hits)\")\n",
    "    \n",
    "    # Average improvement across all systems\n",
    "    avg_improvement = np.mean([\n",
    "        ((m['total_semantic_hits'] / m['total_exact_hits']) - 1) * 100 \n",
    "        for m in system_metrics.values() \n",
    "        if m and m['total_exact_hits'] > 0\n",
    "    ])\n",
    "    print(f\"• Average Improvement: Semantic matching finds {avg_improvement:.0f}% more hits than exact matching\")\n",
    "    \n",
    "    total_semantic = sum(m['total_semantic_hits'] for m in system_metrics.values() if m)\n",
    "    total_exact = sum(m['total_exact_hits'] for m in system_metrics.values() if m)\n",
    "    print(f\"• Overall: {total_semantic} semantic hits vs {total_exact} exact hits across all systems\")\n",
    "    \n",
    "    print(\"=\" * 160)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 10. Show Examples for Each System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.647581Z",
     "iopub.status.busy": "2025-07-07T14:27:37.647465Z",
     "iopub.status.idle": "2025-07-07T14:27:37.661793Z",
     "shell.execute_reply": "2025-07-07T14:27:37.661480Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_system_examples(system_name, results, num_examples=5):\n",
    "    \"\"\"\n",
    "    Show example results for a system.\n",
    "    \"\"\"\n",
    "    display_name = system_display_names.get(system_name, system_name)\n",
    "    \n",
    "    if not results:\n",
    "        print(f\"\\n=== {display_name} - No Results Available ===\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n=== {display_name} - Top {num_examples} Examples ===\")\n",
    "    \n",
    "    # Sort by number of semantic hits (best first)\n",
    "    results_sorted = sorted(results, key=lambda x: x['num_semantic_hits'], reverse=True)\n",
    "    \n",
    "    for i, result in enumerate(results_sorted[:num_examples]):\n",
    "        print(f\"\\n--- Example {i+1} (Sound ID: {result['sound_id']}) ---\")\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Ground Truth Tags: {result['ground_truth_tags']}\")\n",
    "        print(f\"Predicted Tags:\")\n",
    "        \n",
    "        for j, tag in enumerate(result['predicted_tags'], 1):\n",
    "            tag_normalized = tag.lower().replace('-', ' ')\n",
    "            semantic_hit_marker = \"🔮\" if tag_normalized in result['semantic_hits'] else \" \"\n",
    "            exact_hit_marker = \"★\" if tag_normalized in result['exact_hits'] else \" \"\n",
    "            \n",
    "            # Show prediction score if available\n",
    "            score_str = \"\"\n",
    "            if 'prediction_scores' in result and j <= len(result['prediction_scores']):\n",
    "                score = result['prediction_scores'][j-1]\n",
    "                score_str = f\" ({score:.4f})\"\n",
    "            \n",
    "            print(f\"  {j:2d}.{semantic_hit_marker}{exact_hit_marker} {tag:<20}{score_str}\")\n",
    "        \n",
    "        print(f\"\\nSemantic Matches (threshold={SIMILARITY_THRESHOLD}):\")\n",
    "        if result['semantic_matches']:\n",
    "            for match in result['semantic_matches']:\n",
    "                print(f\"  '{match['predicted']}' ↔ '{match['matched_gt']}' (similarity: {match['similarity']:.3f})\")\n",
    "        else:\n",
    "            print(\"  No semantic matches found\")\n",
    "        \n",
    "        print(f\"\\nExact hits: {result['exact_hits']} ({result['num_exact_hits']} hits)\")\n",
    "        print(f\"Semantic hits: {result['semantic_hits']} ({result['num_semantic_hits']} hits)\")\n",
    "        print(f\"Precision@10: {result['precision_at_10']:.3f}, Recall: {result['recall']:.3f}, F1: {result['f1_score']:.3f}\")\n",
    "\n",
    "# Show examples for all systems\n",
    "for system_name in systems_order:\n",
    "    if system_name in sbert_evaluated_results and sbert_evaluated_results[system_name]:\n",
    "        show_system_examples(system_name, sbert_evaluated_results[system_name], num_examples=5)\n",
    "    else:\n",
    "        display_name = system_display_names.get(system_name, system_name)\n",
    "        print(f\"\\n=== {display_name} - No Results Available ===\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 11. Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.663269Z",
     "iopub.status.busy": "2025-07-07T14:27:37.663145Z",
     "iopub.status.idle": "2025-07-07T14:27:37.704749Z",
     "shell.execute_reply": "2025-07-07T14:27:37.704438Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "def convert_numpy_types(obj):\n",
    "    \"\"\"Recursively convert numpy types to Python types\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Prepare comprehensive comparison data\n",
    "comparison_data = {\n",
    "    'metadata': {\n",
    "        'sbert_model': 'all-MiniLM-L6-v2',\n",
    "        'similarity_threshold': float(SIMILARITY_THRESHOLD),\n",
    "        'evaluation_date': pd.Timestamp.now().isoformat(),\n",
    "        'normalization': 'lowercase + hyphens to spaces'\n",
    "    },\n",
    "    'system_metrics': convert_numpy_types(system_metrics),\n",
    "    'detailed_results': convert_numpy_types(sbert_evaluated_results)\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "output_file = f'eval/rankst_clap_sbert_comparison_threshold{SIMILARITY_THRESHOLD}.json'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(comparison_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nComprehensive results saved to {output_file}\")\n",
    "\n",
    "# Save summary only\n",
    "summary_data = {\n",
    "    'metadata': comparison_data['metadata'],\n",
    "    'system_metrics': comparison_data['system_metrics']\n",
    "}\n",
    "\n",
    "summary_file = f'eval/rankst_clap_sbert_summary_threshold{SIMILARITY_THRESHOLD}.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary_data, f, indent=2)\n",
    "\n",
    "print(f\"Summary saved to {summary_file}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 12. Final Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T14:27:37.706427Z",
     "iopub.status.busy": "2025-07-07T14:27:37.706314Z",
     "iopub.status.idle": "2025-07-07T14:27:37.717920Z",
     "shell.execute_reply": "2025-07-07T14:27:37.717601Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY - SBERT Semantic Evaluation\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Similarity Threshold: {SIMILARITY_THRESHOLD}\")\n",
    "\n",
    "# Filter out empty metrics\n",
    "valid_metrics = {k: v for k, v in system_metrics.items() if v}\n",
    "print(f\"Evaluation completed for {len(valid_metrics)} systems\")\n",
    "\n",
    "if valid_metrics:\n",
    "    # Find best performing system\n",
    "    best_f1_system = max(valid_metrics.items(), key=lambda x: x[1]['avg_f1_score'])\n",
    "    best_precision_system = max(valid_metrics.items(), key=lambda x: x[1]['avg_precision_at_10'])\n",
    "    best_recall_system = max(valid_metrics.items(), key=lambda x: x[1]['avg_recall'])\n",
    "    best_hit_rate_system = max(valid_metrics.items(), key=lambda x: x[1]['semantic_hit_rate'])\n",
    "\n",
    "    print(f\"\\nBest F1 Score: {system_display_names.get(best_f1_system[0], best_f1_system[0])} ({best_f1_system[1]['avg_f1_score']:.4f})\")\n",
    "    print(f\"Best Precision@10: {system_display_names.get(best_precision_system[0], best_precision_system[0])} ({best_precision_system[1]['avg_precision_at_10']:.4f})\")\n",
    "    print(f\"Best Recall: {system_display_names.get(best_recall_system[0], best_recall_system[0])} ({best_recall_system[1]['avg_recall']:.4f})\")\n",
    "    print(f\"Best Hit Rate: {system_display_names.get(best_hit_rate_system[0], best_hit_rate_system[0])} ({best_hit_rate_system[1]['semantic_hit_rate']:.1%})\")\n",
    "else:\n",
    "    print(\"\\nNo valid system metrics found. Please ensure the evaluation files exist:\")\n",
    "    for system_name, file_path in evaluation_files.items():\n",
    "        exists = \"✓\" if os.path.exists(file_path) else \"✗\"\n",
    "        print(f\"  {exists} {file_path}\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug the F1 improvement calculation for CLAP Baseline\n",
    "print(\"=== DEBUGGING F1 IMPROVEMENT CALCULATION ===\")\n",
    "\n",
    "clap_baseline_metrics = system_metrics['clap_baseline']\n",
    "print(f\"System: CLAP Baseline\")\n",
    "print(f\"Raw metrics object keys: {list(clap_baseline_metrics.keys())}\")\n",
    "\n",
    "# Calculate exact matching F1 (same way as in the table display)\n",
    "exact_precision = clap_baseline_metrics['total_exact_hits'] / (clap_baseline_metrics['num_clips'] * 10)\n",
    "exact_recall = clap_baseline_metrics['total_exact_hits'] / clap_baseline_metrics['total_ground_truth']\n",
    "exact_f1 = 2 * (exact_precision * exact_recall) / (exact_precision + exact_recall) if (exact_precision + exact_recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nExact Matching Calculation:\")\n",
    "print(f\"  exact_precision = {clap_baseline_metrics['total_exact_hits']} / ({clap_baseline_metrics['num_clips']} * 10) = {exact_precision:.6f}\")\n",
    "print(f\"  exact_recall = {clap_baseline_metrics['total_exact_hits']} / {clap_baseline_metrics['total_ground_truth']} = {exact_recall:.6f}\")\n",
    "print(f\"  exact_f1 = 2 * ({exact_precision:.6f} * {exact_recall:.6f}) / ({exact_precision:.6f} + {exact_recall:.6f}) = {exact_f1:.6f}\")\n",
    "\n",
    "print(f\"\\nSemantic Matching:\")\n",
    "print(f\"  semantic F1 = {clap_baseline_metrics['overall_f1_score']:.6f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "f1_improvement = ((clap_baseline_metrics['overall_f1_score'] / exact_f1) - 1) * 100 if exact_f1 > 0 else float('inf')\n",
    "print(f\"\\nF1 Improvement Calculation:\")\n",
    "print(f\"  f1_improvement = (({clap_baseline_metrics['overall_f1_score']:.6f} / {exact_f1:.6f}) - 1) * 100\")\n",
    "print(f\"  f1_improvement = ({clap_baseline_metrics['overall_f1_score'] / exact_f1:.6f} - 1) * 100\")\n",
    "print(f\"  f1_improvement = {f1_improvement:.2f}%\")\n",
    "\n",
    "# Manual verification\n",
    "manual_calc = (0.016 / 0.007 - 1) * 100\n",
    "print(f\"\\nManual verification (using displayed rounded values):\")\n",
    "print(f\"  (0.016 / 0.007 - 1) * 100 = {manual_calc:.2f}%\")\n",
    "\n",
    "print(f\"\\nDISCREPANCY ANALYSIS:\")\n",
    "print(f\"  Table shows: +131%\")\n",
    "print(f\"  Code calculates: +{f1_improvement:.0f}%\") \n",
    "print(f\"  Manual calc with displayed values: +{manual_calc:.0f}%\")\n",
    "print(f\"  Difference between code and manual: {abs(f1_improvement - manual_calc):.2f} percentage points\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clap2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
